<!doctype html>
<html lang="zh-CN">
<head>
<title>6-Apache Hive SQL-DDL、DML</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<script async type="module">import mermaid from"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs"</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js" integrity="sha512-hpZ5pDCF2bRCweL5WoA0/N1elet1KYL5mx3LP555Eg/0ZguaHawxNvEjF6O3rufAChs16HVNhEc6blF/rZoowQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-sv0slik/5O0JIPdLBCR2A3XDg/1U3WuDEheZfI/DI5n8Yqc3h5kjrnr46FGBNiUAJF7rE4LHKwQ/SoSLRKAxEA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script async src="https://cdn.jsdelivr.net/npm/lucide@0.115.0/dist/umd/lucide.min.js"></script>
<script>window.addEventListener("load",(()=>{document.querySelectorAll(".callout").forEach((e=>{const t=getComputedStyle(e).getPropertyValue("--callout-icon"),l=t&&t.trim().replace(/^lucide-/,"");if(l){const t=e.querySelector(".callout-title");if(t){const e=document.createElement("div"),c=document.createElement("i");e.appendChild(c),c.setAttribute("icon-name",l),e.setAttribute("class","callout-icon"),t.insertBefore(e,t.firstChild)}}})),lucide.createIcons(),Array.from(document.querySelectorAll(".callout.is-collapsible")).forEach((e=>{e.querySelector(".callout-title").addEventListener("click",(t=>{e.classList.contains("is-collapsed")?e.classList.remove("is-collapsed"):e.classList.add("is-collapsed")}))}))}))</script>
<script async src="https://fastly.jsdelivr.net/npm/force-graph@1.43.0/dist/force-graph.min.js"></script>
<script async src="https://fastly.jsdelivr.net/npm/@alpinejs/persist@3.11.1/dist/cdn.min.js"></script>
<script src="https://fastly.jsdelivr.net/npm/alpinejs@3.11.1/dist/cdn.min.js" async></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism-okaidia.min.css" integrity="sha512-mIs9kKbaw6JZFfSuo+MovjU+Ntggfoj8RwAmJbVXQ5mkAX5LlgETQEweFPI18humSPHymTb5iikEOKWF7I8ncQ==" crossorigin="anonymous" referrerpolicy="no-referrer" async>
<script src="https://fastly.jsdelivr.net/npm/whatwg-fetch@3.6.2/dist/fetch.umd.min.js" crossorigin="anonymous" referrerpolicy="no-referrer" async></script>
<link href="/styles/digital-garden-base.css" rel="stylesheet">
<link href="/styles/obsidian-base.css" rel="stylesheet">
<link href="/styles/_theme.74516f71.css" rel="stylesheet">
<link href="/styles/custom-style.css" rel="stylesheet">
<link rel="icon" href="/favicon.ico" sizes="any">
<link rel="icon" href="/favicon.svg" type="image/svg+xml">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="manifest" href="/manifest.webmanifest">
<style></style>
<style></style>
</head>
<body class="theme-light markdown-preview-view markdown-rendered markdown-preview-section css-settings-manager mod-windows is-frameless is-maximized is-hidden-frameless is-focused obsidian-app theme-light show-inline-title show-ribbon show-view-header css-settings-manager theme-default line-style-solid folder-default blockquote-normal callout-normal checkbox-default tag-default link-default heading-default responsive-tile-height oz-show-all-num">
<nav class="navbar">
<div class="navbar-inner">
<a href="/" style="text-decoration:none">
<h1 style="margin:15px!important">czc&#39;s digital garden</h1>
</a>
</div>
<div class="search-button align-icon" onclick="toggleSearch()">
<span class="search-icon">
<i icon-name="search"></i>
</span>
<span class="search-text">
<span>Search</span>
<span style="font-size:.6rem;padding:2px 2px 0 6px;text-align:center;transform:translateY(4px)" class="search-keys">
CTRL + K
</span>
</span>
</div>
</nav>
<div class="search-container" id="globalsearch" onclick="toggleSearch()">
<div class="search-box">
<input type="search" id="term" placeholder="Start typing...">
<div id="search-results"></div>
<footer class="search-box-footer">
<div class="navigation-hint">
<span>Enter to select</span>
</div>
<div class="navigation-hint align-icon">
<i icon-name="arrow-up" aria-hidden="true"></i>
<i icon-name="arrow-down" aria-hidden="true"></i>
<span>to navigate</span>
</div>
<div class="navigation-hint">
<span>ESC to close</span>
</div>
</footer>
</div>
</div>
<script src="https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js"></script>
<script>document.addEventListener("DOMContentLoaded",init,!1),document.addEventListener("DOMContentLoaded",setCorrectShortcut,!1),window.toggleSearch=function(){document.getElementById("globalsearch").classList.contains("active")?document.getElementById("globalsearch").classList.remove("active"):(document.getElementById("globalsearch").classList.add("active"),document.getElementById("term").focus())},window.toggleTagSearch=function(e){console.log(e.textContent);const t=e.textContent;t&&(window.document.getElementById("term").value=t.trim(),window.toggleSearch(),window.search())};const loadingSvg='\n    <svg width="100" height="100" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg" stroke="#fff">\n      <g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2">\n          <circle cx="22" cy="22" r="6" stroke-opacity="0">\n              <animate attributeName="r"\n                   begin="1.5s" dur="3s"\n                   values="6;22"\n                   calcMode="linear"\n                   repeatCount="indefinite" />\n              <animate attributeName="stroke-opacity"\n                   begin="1.5s" dur="3s"\n                   values="1;0" calcMode="linear"\n                   repeatCount="indefinite" />\n              <animate attributeName="stroke-width"\n                   begin="1.5s" dur="3s"\n                   values="2;0" calcMode="linear"\n                   repeatCount="indefinite" />\n          </circle>\n          <circle cx="22" cy="22" r="6" stroke-opacity="0">\n              <animate attributeName="r"\n                   begin="3s" dur="3s"\n                   values="6;22"\n                   calcMode="linear"\n                   repeatCount="indefinite" />\n              <animate attributeName="stroke-opacity"\n                   begin="3s" dur="3s"\n                   values="1;0" calcMode="linear"\n                   repeatCount="indefinite" />\n              <animate attributeName="stroke-width"\n                   begin="3s" dur="3s"\n                   values="2;0" calcMode="linear"\n                   repeatCount="indefinite" />\n          </circle>\n          <circle cx="22" cy="22" r="8">\n              <animate attributeName="r"\n                   begin="0s" dur="1.5s"\n                   values="6;1;2;3;4;5;6"\n                   calcMode="linear"\n                   repeatCount="indefinite" />\n          </circle>\n      </g>\n  </svg>';function debounce(e,t,n){var a;return function(){var r=this,i=arguments,c=n&&!a;clearTimeout(a),a=setTimeout((function(){a=null,n||e.apply(r,i)}),t),c&&e.apply(r,i)}}function setCorrectShortcut(){navigator.platform.toUpperCase().indexOf("MAC")>=0&&document.querySelectorAll(".search-keys").forEach((e=>e.innerHTML="⌘ + K"))}function createIndex(e){const t=e=>e.toLowerCase().split(/([^a-z]|[^\x00-\x7F])/),n=new FlexSearch.Document({cache:!0,charset:"latin:extra",optimize:!0,index:[{field:"content",tokenize:"reverse",encode:t},{field:"title",tokenize:"forward",encode:t},{field:"tags",tokenize:"forward",encode:t}]});return e.forEach(((e,t)=>{n.add({id:t,title:e.title,content:e.content,tags:e.tags})})),n}async function init(){let e=!0;if(localStorage.getItem("searchIndex")){let{date:t,docs:n}=JSON.parse(localStorage.getItem("searchIndex"));if("2025-06-29T04:38:31.215Z"===t){e=!1;let t=createIndex(n);window.docs=n,window.index=t}}if(e){let e=await(await fetch("/searchIndex.json?v=2025-06-29T04:38:31.215Z")).json(),t=createIndex(e);localStorage.setItem("searchIndex",JSON.stringify({date:"2025-06-29T04:38:31.215Z",docs:e})),window.docs=e,window.index=t}document.addEventListener("keydown",(e=>{if((e.ctrlKey||e.metaKey)&&"k"===e.key&&(e.preventDefault(),toggleSearch()),"Escape"===e.key&&document.getElementById("globalsearch").classList.remove("active"),document.getElementById("globalsearch").classList.contains("active")){if("ArrowDown"===e.key){e.preventDefault();let t=document.querySelector(".searchresult.active");t?(t.classList.remove("active"),t.nextElementSibling?t.nextElementSibling.classList.add("active"):document.querySelector(".searchresult").classList.add("active")):document.querySelector(".searchresult").classList.add("active");let n=document.querySelector(".searchresult.active");n&&n.scrollIntoView({behavior:"smooth",block:"nearest",inline:"start"})}if("ArrowUp"===e.key){e.preventDefault();let t=document.querySelector(".searchresult.active");t?(t.classList.remove("active"),t.previousElementSibling?t.previousElementSibling.classList.add("active"):document.querySelectorAll(".searchresult").forEach((e=>{e.nextElementSibling||e.classList.add("active")}))):document.querySelectorAll(".searchresult").forEach((e=>{e.nextElementSibling&&e.classList.add("active")}));let n=document.querySelector(".searchresult.active");n&&n.scrollIntoView({behavior:"smooth",block:"nearest",inline:"start"})}if("Enter"===e.key){e.preventDefault();let t=document.querySelector(".searchresult.active");t&&(window.location.href=t.querySelector("a").href)}}}));const t=debounce(search,200,!1);field=document.querySelector("#term"),field.addEventListener("keydown",(e=>{"ArrowDown"!==e.key&&"ArrowUp"!==e.key&&t()})),resultsDiv=document.querySelector("#search-results");const n=new URL(location.href).searchParams;n.get("q")&&(field.setAttribute("value",n.get("q")),toggleSearch(),search())}async function search(){let e=field.value.trim();if(!e)return;if(e==lastSearch)return;console.log(`search for ${e}`),window.lastSearch=e,resultsDiv.innerHTML=loadingSvg;let t=offlineSearch(e),n="";if(!t.length){let t=document.createElement("p");return t.innerText=`No results for "${e}"`,resultsDiv.innerHTML="",void resultsDiv.appendChild(t)}n+='<div style="max-width:100%;">',t.forEach((e=>{e.tags&&e.tags.length>0?n+=`<div class="searchresult">\n                    <a class="search-link" href="${e.url}">${e.title}</a>\n                    <div onclick="window.location='${e.url}'">\n                        <div class="header-meta">\n                            <div class="header-tags">\n                                ${e.tags.map((e=>'<a class="tag" href="JavaScript:Void(0);">#'+e+"</a>")).join("")}\n                            </div>\n                        </div>\n                        ${e.content}\n                    </div>\n                </div>`:n+=`<div class="searchresult">\n                    <a class="search-link" href="${e.url}">${e.title}</a>\n                    <div onclick="window.location='${e.url}'">\n                        ${e.content}\n                    </div>\n                </div>`})),n+="</div>",resultsDiv.innerHTML=n}function truncate(e,t){return(e=e.replaceAll(/<[^>]*>/g,"")).length<t?e:e.substring(0,t-3)+"..."}function offlineSearch(e){let t=window.docs,n="#"===e[0]&&e.length>1?index.search(e.substring(1),[{field:"tags"}]):index.search(e,[{field:"title",limit:5},{field:"content",weight:10}]);const a=e=>{const t=n.filter((t=>t.field===e));return 0===t.length?[]:[...t[0].result]};return[...new Set([...a("title"),...a("content"),...a("tags")])].map((e=>{let n=t[e];return n.content=truncate(n.content,400),n.tags=n.tags.filter((e=>"gardenEntry"!=e&&"note"!=e)),n}))}window.lastSearch=""</script>
<main class="content cm-s-obsidian">
<header>
<h1 data-note-icon="">6-Apache Hive SQL-DDL、DML</h1>
<div class="header-meta">
<div class="header-tags">
</div>
<div class="timestamps"><div><i icon-name="calendar-plus"></i> <span class="human-date" data-date="2025-06-09T10:30:37.832+08:00"></span></div><div><i icon-name="calendar-clock"></i> <span class="human-date" data-date="2025-03-22T11:04:53.000+08:00"></span></div></div></div>
</header>
<h3 id="hadoop-day06-apache-hive-sql-ddl-dml" tabindex="-1">hadoop离线day06-Apache Hive SQL-DDL、DML</h3>
<hr>
<h4 id="今日课程学习目标" tabindex="-1">今日课程学习目标</h4>
<pre><code class="language-shell"><a class="tag" onclick="toggleTagSearch(this)" data-content="#掌握HQL">#掌握HQL</a> DDL建表语句
	理解Hive SerDe机制、分隔符语法
	掌握内外部表、分区表、分桶表创建使用
<a class="tag" onclick="toggleTagSearch(this)" data-content="#理解HQL">#理解HQL</a> DDL其他语句
	修改、删除
<a class="tag" onclick="toggleTagSearch(this)" data-content="#学会常见show语法使用">#学会常见show语法使用</a>
<a class="tag" onclick="toggleTagSearch(this)" data-content="#掌握HQL">#掌握HQL</a> load、insert加载插入语句
</code></pre>
<h4 id="今日课程内容大纲" tabindex="-1">今日课程内容大纲</h4>
<pre><code class="language-shell"><a class="tag" onclick="toggleTagSearch(this)" data-content="#1、HQL">#1、HQL</a> DDL 数据定义语言 针对表的
 核心：建表语句   直接决定了表和文件之间能否映射成功
 	数据类型
 	SerDe序列化机制
 	分隔符语法
 	内部表、外部表
 	数据存储路径
 	分区表
 	分桶表
  alter修改表

<a class="tag" onclick="toggleTagSearch(this)" data-content="#2、HQL">#2、HQL</a> DML 数据操纵语言  针对表的数据
	load加载数据
	insert插入数据   insert+select
		多重插入
		动态分区插入
		数据导出
		
<a class="tag" onclick="toggleTagSearch(this)" data-content="#3、HQL常用的show语法">#3、HQL常用的show语法</a>		
</code></pre>
<hr>
<h4 id="01-apache-hive-ddl" tabindex="-1">知识点01：Apache Hive--DDL--概念与语法树介绍</h4>
<p><picture src="/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E8%AE%A1%E7%AE%97%E6%9C%BA/Hadoop%E6%8A%80%E6%9C%AF%E6%A0%88/%E8%B5%84%E6%96%99%E8%AE%B2%E4%B9%89/%E6%BA%90/day06--Apache%20Hive%20SQL-DDL%E3%80%81DML/1%E3%80%81%E7%AC%94%E8%AE%B0%E3%80%81%E6%80%BB%E7%BB%93/hadoop%E7%A6%BB%E7%BA%BFday06-Apache%20Hive%20SQL-DDL%E3%80%81DML.assets/image-20210921211129630.png" alt="image-20210921211129630.png"><source media="(max-width:480px)" srcset="/img/optimized/UMOct1on_U-500.webp" type="image/webp">
<source media="(max-width:480px)" srcset="/img/optimized/UMOct1on_U-500.jpeg">
<source media="(max-width:1920px)" srcset="/img/optimized/UMOct1on_U-700.webp" type="image/webp"><source media="(max-width:1920px)" srcset="/img/optimized/UMOct1on_U-700.jpeg"><img class="" src="/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E8%AE%A1%E7%AE%97%E6%9C%BA/Hadoop%E6%8A%80%E6%9C%AF%E6%A0%88/%E8%B5%84%E6%96%99%E8%AE%B2%E4%B9%89/%E6%BA%90/day06--Apache%20Hive%20SQL-DDL%E3%80%81DML/1%E3%80%81%E7%AC%94%E8%AE%B0%E3%80%81%E6%80%BB%E7%BB%93/hadoop%E7%A6%BB%E7%BA%BFday06-Apache%20Hive%20SQL-DDL%E3%80%81DML.assets/image-20210921211129630.png" alt="image-20210921211129630.png" width=""></picture></p>
<pre><code class="language-properties">蓝色字体是建表语法的关键字，用于指定某些功能。
[ ]中括号的语法表示可选。
|表示使用的时候，左右语法二选一。
建表语句中的语法顺序要和语法树中顺序保持一致。

</code></pre>
<hr>
<h4 id="02-apache-hive-ddl" tabindex="-1">知识点02：Apache Hive--DDL--建表语句--表存在忽略异常</h4>
<p><mark>IF NOT EXISTS</mark></p>
<ul>
<li>
<p>建表的时候，如果表名已经存在，默认会报错，通过<mark>IF NOT EXISTS关键字可以忽略异常</mark>。</p>
<pre><code class="language-sql">--第一次创建表
0: jdbc:hive2://node1:10000&gt; create table t_1(id int,name string,age int);
--再次执行
0: jdbc:hive2://node1:10000&gt; create table t_1(id int,name string,age int);
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table t_1 already exists) (state=08S01,code=1)
--Error while processing statement 执行期间的错误 往往就是逻辑错误

--加上if not exists忽略异常
0: jdbc:hive2://node1:10000&gt; create table if not exists t_1(id int,name string,age int);

0: jdbc:hive2://node1:10000&gt; creatf table t_1(id int,name string,age int);
Error: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'creatf' 'table' 't_1' (state=42000,code=40000)
0: jdbc:hive2://node1:10000&gt; 
--Error while compiling statement 编译期间的错误  SQL语法问题

hivesql --&gt;编译---&gt;执行
</code></pre>
</li>
</ul>
<hr>
<h4 id="03-apache-hive-ddl" tabindex="-1">知识点03：Apache Hive--DDL--建表语句--数据类型</h4>
<p>hive语法手册</p>
<blockquote>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types" target="_blank" class="external-link">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types</a></p>
</blockquote>
<ul>
<li>
<p>Hive除了支持SQL类型之外，<mark>还支持java数据类型</mark>；</p>
</li>
<li>
<p>Hive除了支持基础数据类型之外，还<mark>支持复合类型</mark>（array数组 map映射）；</p>
<ul>
<li>针对复合类型的数据 要想直接从文件中解析成功 还必须配合分隔符指定的语法。</li>
</ul>
</li>
<li>
<p>Hive中大小写不敏感；</p>
</li>
<li>
<p>在建表的时候，最好<mark>表的字段类型要和文件中的类型保持一致</mark>，</p>
<ul>
<li>如果不一致，Hive会尝试进行<mark>类型隐式转换</mark>，不保证转换成功，如果不成功，显示null;</li>
</ul>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">--创建数据库并切换使用
create database if not exists itheima;
use itheima;

--建表
create table t_archer(
      id int comment &quot;ID&quot;,
      name string comment &quot;英雄名称&quot;,
      hp_max int comment &quot;最大生命&quot;,
      mp_max int comment &quot;最大法力&quot;,
      attack_max int comment &quot;最高物攻&quot;,
      defense_max int comment &quot;最大物防&quot;,
      attack_range string comment &quot;攻击范围&quot;,
      role_main string comment &quot;主要定位&quot;,
      role_assist string comment &quot;次要定位&quot;
) comment &quot;王者荣耀射手信息&quot;
row format delimited
fields terminated by &quot;\t&quot;;

--查看表数据 查看表的元数据信息
select * from t_archer;
desc formatted t_archer;

--上传文件到表对应的HDFS目录下
[root@node1 hivedata]# hadoop fs -put archer.txt /user/hive/warehouse/itheima.db/t_archer

[root@node1 hiedata]# pwd
/root/hivedata
</code></pre>
</li>
</ul>
<hr>
<h4 id="04-apache-hive-ddl-ser-de" tabindex="-1">知识点04：Apache Hive--DDL--建表语句--SerDe机制、分隔符指定语法</h4>
<ul>
<li>
<p>机制：<mark>SerDe（Serializer and Deserializer）序列化机制</mark></p>
</li>
<li>
<p>Hive使用SerDe机制读写HDFS上文件</p>
</li>
<li>
<p>读文件</p>
<ul>
<li>
<p>HDFS files --&gt; InputFileFormat --&gt; &lt;key, value&gt; --&gt; <mark>Deserializer</mark> --&gt; Row object</p>
<pre><code class="language-shell"><a class="tag" onclick="toggleTagSearch(this)" data-content="#1、使用InputFileFormat（默认实现TextInputFormat）读取hdfs上文件">#1、使用InputFileFormat（默认实现TextInputFormat）读取hdfs上文件</a>
    一行一行读取数据，返回&lt;k，v&gt;键值对类型
<a class="tag" onclick="toggleTagSearch(this)" data-content="#2、返回">#2、返回</a>&lt;key, value&gt;，其中数据存储在value中

<a class="tag" onclick="toggleTagSearch(this)" data-content="#3、使用Deserializer反序列化动作读取value">#3、使用Deserializer反序列化动作读取value</a>  解析成为对象（Row object）
    默认的序列化类LazysimpleSerDe
</code></pre>
</li>
</ul>
</li>
<li>
<p>写文件</p>
<ul>
<li>Row object --&gt; <mark>Serializer</mark> --&gt; &lt;key, value&gt; --&gt; OutputFileFormat --&gt; HDFS files</li>
</ul>
</li>
<li>
<p>分隔符指定语法</p>
<ul>
<li>
<p>语法格式</p>
<pre><code>ROW FORMAT DELIMITED | SERDE

ROW FORMAT DELIMITED  表示使用LazySimpleSerDe类进行序列化解析数据
 
ROW FORMAT SERDE      表示使用其他SerDe类进行序列化解析数据
</code></pre>
</li>
<li>
<p>ROW FORMAT DELIMITED具体的子语法</p>
<pre><code class="language-shell">row format delimited
[fields terminated by char]  <a class="tag" onclick="toggleTagSearch(this)" data-content="#指定字段之间的分隔符">#指定字段之间的分隔符</a>
[collection items terminated by char]  <a class="tag" onclick="toggleTagSearch(this)" data-content="#指定集合元素之间的分隔符">#指定集合元素之间的分隔符</a>
[map keys terminated by char]         <a class="tag" onclick="toggleTagSearch(this)" data-content="#指定map类型kv之间的分隔符">#指定map类型kv之间的分隔符</a>
[lines terminated by char]            <a class="tag" onclick="toggleTagSearch(this)" data-content="#指定换行符">#指定换行符</a>
</code></pre>
</li>
<li>
<p>课堂练习sql</p>
<pre><code class="language-sql">create table t_hot_skin_1(
id int,
name string,
win_rate int,
skin_price string
)
row format delimited
fields terminated by ',';


create table t_hot_skin_2(
id int,
name string,
win_rate int,
skin_price map&lt;string,int&gt;
)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':';
</code></pre>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="05-apache-hive-ddl" tabindex="-1">知识点05：Apache Hive--DDL--建表语句--默认分隔符</h4>
<ul>
<li>
<p>默认分隔符</p>
<ul>
<li>Hive在建表的时候，如果没有row format语法，则该表使用==\001默认分隔符==进行字段分割；</li>
<li>如果此时文件中的数据字段之间的分隔符也是\001 ，那么就可以直接映射成功。</li>
<li><mark>针对默认分隔符，其是一个不可见分隔符，在代码层面是\001表示</mark></li>
<li>在vim编辑器中，连续输入ctrl+v 、ctrl+a；</li>
<li>在实际工作中，Hive最喜欢的就是\001分隔符，在清洗数据的时候，<mark>有意识</mark>的把数据之间的分隔符指定为\001;</li>
</ul>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">--建表
create table t_hot_hero_skin_price(
      id int,
      name string,
      win_rate int,
      skin_price map&lt;string,int&gt;
)
row format delimited
fields terminated by ',' --字段之间分隔符
collection items terminated by '-'  --集合元素之间分隔符
map keys terminated by ':'; --集合元素kv之间分隔符;

--上传数据
hadoop fs -put hot_hero_skin_price.txt /user/hive/warehouse/itheima.db/t_hot_hero_skin_price 

select *
from t_hot_hero_skin_price;


--有点想法： 就把map数据类型当成字符串映射如何？
create table t_hot_hero_skin_price_str(
      id int,
      name string,
      win_rate int,
      skin_price string
)
row format delimited
fields terminated by ',';

hadoop fs -put hot_hero_skin_price.txt /user/hive/warehouse/itheima.db/t_hot_hero_skin_price_str 

--结论 
	·1、不管使用map还是使用string来定义数据 都能解析映射成功
	 2、区别在于使用的过程中 一个是针对map类型数据处理  一个是针对string类型的数据处理
	 
	 
select skin_price from t_hot_hero_skin_price;	 
select skin_price from t_hot_hero_skin_price_str;


select skin_price[&quot;至尊宝&quot;] from t_hot_hero_skin_price limit 1;	 
select skin_price[&quot;至尊宝&quot;] from t_hot_hero_skin_price_str limit 1; --语法错误


--建表 不指定分隔符
create table t_team_ace_player(
   id int,
   team_name string,
   ace_player_name string
); --没有指定row format语句 此时采用的是默认的\001作为字段的分隔符

hadoop fs -put team_ace_player.txt /user/hive/warehouse/itheima.db/t_team_ace_player

select * from t_team_ace_player;
</code></pre>
</li>
</ul>
<hr>
<h4 id="06-apache-hive-ddl" tabindex="-1">知识点06：Apache Hive--DDL--建表语句--内部表、外部表</h4>
<pre><code class="language-sql">--创建内部表
create table student_inner(Sno int,Sname string,Sex string,Sage int,Sdept string) row format delimited fields terminated by ',';

--创建外部表 关键字external
create external table student_external(Sno int,Sname string,Sex string,Sage int,Sdept string) row format delimited fields terminated by ',';

--上传文件到内部表、外部表中
hadoop fs -put students.txt /user/hive/warehouse/itheima.db/student_inner
hadoop fs -put students.txt /user/hive/warehouse/itheima.db/student_external

--好像没啥区别 都能映射成功 数据也都在HDFS上

--针对内部表、外部表 进行drop删除操作
drop table student_inner;  --内部表在删除的时候 元数据和数据都会被删除
drop table student_external; --外部表在删除的时候 只删除元数据  而HDFS上的数据文件不会动
</code></pre>
<ul>
<li>
<p>外部表有什么好处</p>
<pre><code>最大的好处是防止误操作删除表的时候 把表的数据一起删除。
</code></pre>
</li>
<li>
<p>可以通过命令去查询表的元数据信息 获取表的类型</p>
<pre><code class="language-sql">desc formatted table_name;

MANAGED_TABLE     内部表、受控表(所谓的受控指定是表对应的HDFS上的数据受到我的控制)
EXTERNAL_TABLE    外部表
</code></pre>
</li>
</ul>
<hr>
<h4 id="07-apache-hive-ddl-location" tabindex="-1">知识点07：Apache Hive--DDL--建表语句--location存储位置</h4>
<blockquote>
<p>存储路径由hive.metastore.warehouse.dir 属性指定。<mark>默认值是：/user/hive/warehouse</mark></p>
</blockquote>
<ul>
<li>
<p>不管是内部表，还是外部表，在HDFS上的路径如下：</p>
<pre><code>/user/hive/warehouse/itcast.db/t_array

/user/hive/warehouse/数据库名.db/表名
</code></pre>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">--在建表的时候 可以使用location关键字指定表的路径在HDFS任意位置
create table t_team_ace_player_location(
 id int,
 team_name string,
 ace_player_name string)
 location '/aaa'; --使用location关键字指定本张表数据在hdfs上的存储路径
 
--此时再上传数据 就必须上传到指定的目录下 否则就解析映射失败了
</code></pre>
</li>
<li>
<p>在实际开发中，最好集中维护管理Hive表数据，避免文件在HDFS随意存放。</p>
</li>
</ul>
<hr>
<h4 id="08-apache-hive-ddl" tabindex="-1">知识点08：Apache Hive--DDL--建表语句--分区表创建、静态数据加载、分区裁剪</h4>
<ul>
<li>分区表的引入背景</li>
</ul>
<pre><code class="language-sql">create table t_all_hero(
   id int,
   name string,
   hp_max int,
   mp_max int,
   attack_max int,
   defense_max int,
   attack_range string,
   role_main string,
   role_assist string
)
row format delimited
fields terminated by &quot;\t&quot;;

--上传文件
hadoop fs -put archer.txt assassin.txt mage.txt support.txt tank.txt warrior.txt /user/hive/warehouse/itheima.db/t_all_hero

select * from t_all_hero;


--查询role_main主要定位是射手并且hp_max最大生命大于6000的有几个

select count(*) from t_all_hero where role_main=&quot;archer&quot; and hp_max &gt;6000;

--思考：上述查询sql底层应该如何去查询数据？ 要不要全表扫描？

--问题：要进行过滤 就需要全表扫描 不全表扫描呢 又得不出正确结果？性能如何优化呢？
--优化要求  如何才能够减少全表扫描 而结果又正确。
</code></pre>
<ul>
<li>分区表的创建</li>
</ul>
<pre><code class="language-sql">--经过大脑分析 我们认为应该根据角色主定位进行分区 所以分区的字段就是role_main
create table t_all_hero_part(
   id int,
   name string,
   hp_max int,
   mp_max int,
   attack_max int,
   defense_max int,
   attack_range string,
   role_main string,
   role_assist string
)
partitioned by(role_main string)
row format delimited
fields terminated by &quot;\t&quot;;



--错误说 分区字段重复了 好家伙
Error: Error while compiling statement: FAILED: SemanticException [Error 10035]: Column repeated in partitioning columns (state=42000,code=10035)


--分区表建表 
create table t_all_hero_part(
   id int,
   name string,
   hp_max int,
   mp_max int,
   attack_max int,
   defense_max int,
   attack_range string,
   role_main string,
   role_assist string
) partitioned by (juesedingwei string)--注意哦 这里是分区字段
row format delimited
fields terminated by &quot;\t&quot;;

--查询分区表 发现分区字段也显示出来了
select * from t_all_hero_part;
</code></pre>
<ul>
<li>分区表的数据加载--静态分区加载</li>
</ul>
<pre><code class="language-sql">--静态加载分区表数据
load data local inpath '/root/hivedata/archer.txt' into table t_all_hero_part partition(juesedingwei='sheshou');

load data local inpath '/root/hivedata/assassin.txt' into table t_all_hero_part partition(juesedingwei='cike');
load data local inpath '/root/hivedata/mage.txt' into table t_all_hero_part partition(juesedingwei='fashi');
load data local inpath '/root/hivedata/support.txt' into table t_all_hero_part partition(juesedingwei='fuzhu');
load data local inpath '/root/hivedata/tank.txt' into table t_all_hero_part partition(juesedingwei='tanke');
load data local inpath '/root/hivedata/warrior.txt' into table t_all_hero_part partition(juesedingwei='zhanshi');

--查询一下验证是否加载成功
select * from t_all_hero_part;

load data local inpath '/root/hivedata/warrior.txt' into table t_all_hero_part partition(juesedingwei='666');

</code></pre>
<blockquote>
<p>思考：如果分区很多 一个一个加载，效率如何？</p>
<p>因为静态分区的时候 分区值是用户手动写死的 有写错的风险。</p>
</blockquote>
<hr>
<h4 id="09-apache-hive-ddl" tabindex="-1">知识点09：Apache Hive--DDL--建表语句--动态分区插入数据</h4>
<ul>
<li>
<p>设置允许动态分区、设置动态分区模式</p>
<pre><code class="language-sql">--动态分区
set hive.exec.dynamic.partition=true; --注意hive3已经默认开启了
set hive.exec.dynamic.partition.mode=nonstrict;

--模式分为strict严格模式  nonstrict非严格模式
严格模式要求 分区字段中至少有一个分区是静态分区。


--举个栗子
set hive.exec.dynamic.partition.mode=strict;  --设置为严格模式


Error: Error while compiling statement: FAILED: SemanticException [Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict (state=42000,code=10096)

--说动态分区在严格模式下 要求至少有一个分区字段为静态分区字段  
--如果不想这么做  就必须把模式设置为非严格


t_1(xxxx) partitioned by (month string,day string);

insert into t_1  partition(month=&quot;202112&quot;,day=&quot;18&quot;) -- 两个分区字段都是静态
insert into t_1  partition(month=&quot;202112&quot;,day)
insert into t_1  partition(month,day=&quot;18&quot;)   --至少一个是静态的
insert into t_1  partition(month,day)  --都是动态的
</code></pre>
</li>
<li>
<p>动态分区加载数据</p>
<blockquote>
<p><mark>insert + select</mark></p>
<p>插入的数据来自于后面的查询语句返回的结果。</p>
<p>查询返回的内容，其字段类型、顺序、个数要和待插入的表保持一致。</p>
</blockquote>
<pre><code class="language-sql">--创建一张新的分区表 t_all_hero_part_dynamic
create table t_all_hero_part_dynamic(
    id int,
    name string,
    hp_max int,
    mp_max int,
    attack_max int,
    defense_max int,
    attack_range string,
    role_main string,
    role_assist string
) partitioned by (role_dong string)
row format delimited
fields terminated by &quot;\t&quot;;

--执行动态分区插入  --注意 分区值并没有手动写死指定
insert into table t_all_hero_part_dynamic partition(role_dong) 
select tmp.*,tmp.role_main from t_all_hero tmp; --查询返回10个字段


--查询验证结果
select * from t_all_hero_part_dynamic;

---严格模式下报错信息
Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict
</code></pre>
<blockquote>
<p>在执行动态分区插入数据的时候，如果是严格模式strict，要求至少一个分区为静态分区？</p>
<p>partition(guojia=&quot;zhongguo&quot;,sheng) --第一个分区写死了（静态） 符合严格模式。</p>
<p>partition(guojia,sheng) --两个分区都是动态确定的 需要非严格模式</p>
</blockquote>
</li>
<li>
<p>分区表的使用--分区裁剪</p>
</li>
</ul>
<pre><code class="language-sql">--非分区表 全表扫描过滤查询
select count(*) from t_all_hero where role_main=&quot;archer&quot; and hp_max &gt;6000;

--分区表 先基于分区过滤 再查询
select count(*) from t_all_hero_part where juesedingwei=&quot;sheshou&quot; and hp_max &gt;6000;
</code></pre>
<hr>
<h4 id="10-apache-hive-ddl" tabindex="-1">知识点10：Apache Hive--DDL--建表语句--多重分区及分区表注意事项</h4>
<p>分区表注意事项</p>
<ul>
<li>
<p><mark>分区表的字段不能是表中已有的字段</mark>；分区的字段也会显示在查询结果上；</p>
</li>
<li>
<p>分区的字段是虚拟的字段，出现在表所有字段的后面，其值来自于加载数据到表中的时候手动指定。</p>
</li>
<li>
<p>分区在底层的形式就是<mark>以文件夹管理不同的文件</mark>；不同文件夹就是表不同分区；文件夹的名字：</p>
<pre><code class="language-shell"></code></pre>
</li>
</ul>
<p>/user/hive/warehouse/数据库.db/表<br>
/分区字段=分区值1<br>
xxxx.txt<br>
/分区字段=分区值2<br>
zzzz.txt</p>
<pre><code>
- 分区表是一种优化表，建表的时候可以不使用，但是，当==创建分区表之后，使用分区字段查询可以减少全表扫描，提高查询的效率==。

- 企业中常用的分区字段

- 地域维度：省，市
- 时间维度：day,month,year

----

多重分区表

- 分区表支持基于多个字段进行分区

</code></pre>
<p>partitioned by(字段1 ，字段2....)</p>
<pre><code>
- 多个分区之间是一种==递进关系==，可以理解为在==前一个分区的基础上继续分区==；从底层来说就是文件夹下面继续划分子文件夹；

- ==常见的多分区就是2个分区==；

```sql
--以国家、省创建分区表
create table t_user_double_p(id int,name string,country string) partitioned by(guojia string,sheng string) row format delimited fields terminated by  ',';

--加载数据到多分区表中
load data local inpath '/root/hivedata/china_sh.txt'  into table t_user_double_p partition(guojia=&quot;zhongguo&quot;,sheng=&quot;shanghai&quot;);

load data local inpath '/root/hivedata/china_sz.txt'  into table t_user_double_p partition(guojia=&quot;zhongguo&quot;,sheng=&quot;shenzhen&quot;);

load data local inpath '/root/hivedata/usa_dezhou.txt'  into table t_user_double_p partition(guojia=&quot;meiguo&quot;,sheng=&quot;dezhou&quot;);

--查询来自于中国深圳的用户有哪些？
select * from t_user_double_p where guojia=&quot;zhongguo&quot;and sheng=&quot;shenzhen&quot;;
</code></pre>
<hr>
<h4 id="11-apache-hive-ddl" tabindex="-1">知识点11：Apache Hive--DDL--建表语句--分桶表语法、创建、加载</h4>
<ul>
<li>从语法层面解析分桶含义</li>
</ul>
<pre><code class="language-sql">CLUSTERED BY xxx INTO N BUCKETS
--根据xxx字段把数据分成N桶
--根据表中的字段把数据文件成为N个部分

t_user(id int,name string);
--1、根据谁分？
 CLUSTERED BY xxx ；  xxx必须是表中的字段
--2、分成几桶？
 N BUCKETS   ；N的值就是分桶的个数
--3、分桶的规则？
clustered by id into 3 bucket

hashfunc(分桶字段)  %  N bucket  余数相同的来到同一个桶中
1、如果分桶的字段是数字类型的字段，hashfunc(分桶字段)=分桶字段本身
2、如果分桶的字段是字符串或者其他字段，hashfunc(分桶字段) = 分桶字段.hashcode
</code></pre>
<ul>
<li>分桶的创建</li>
</ul>
<pre><code class="language-sql">CREATE TABLE itheima.t_usa_covid19_bucket(
      count_date string,
      county string,
      state string,
      fips int,
      cases int,
      deaths int)
CLUSTERED BY(state) INTO 5 BUCKETS; --分桶的字段一定要是表中已经存在的字段

--根据state州分为5桶 每个桶内根据cases确诊病例数倒序排序
CREATE TABLE itheima.t_usa_covid19_bucket_sort(
     count_date string,
     county string,
     state string,
     fips int,
     cases int,
     deaths int)
CLUSTERED BY(state)
sorted by (cases desc) INTO 5 BUCKETS;--指定每个分桶内部根据 cases倒序排序
</code></pre>
<ul>
<li>分桶表的数据加载</li>
</ul>
<pre><code class="language-sql">--step1:开启分桶的功能 从Hive2.0开始不再需要设置
set hive.enforce.bucketing=true;

--step2:把源数据加载到普通hive表中
CREATE TABLE itheima.t_usa_covid19(
       count_date string,
       county string,
       state string,
       fips int,
       cases int,
       deaths int)
row format delimited fields terminated by &quot;,&quot;;

--将源数据上传到HDFS，t_usa_covid19表对应的路径下
hadoop fs -put us-covid19-counties.dat /user/hive/warehouse/itheima.db/t_usa_covid19

--step3:使用insert+select语法将数据加载到分桶表中
insert into t_usa_covid19_bucket select * from t_usa_covid19;

select * from t_usa_covid19_bucket limit 10;
</code></pre>
<ul>
<li>分桶表的使用</li>
</ul>
<pre><code class="language-sql">--基于分桶字段state查询来自于New York州的数据
--不再需要进行全表扫描过滤
--根据分桶的规则hash_function(New York) mod 5计算出分桶编号
--查询指定分桶里面的数据 就可以找出结果  此时是分桶扫描而不是全表扫描
select *
from t_usa_covid19_bucket where state=&quot;New York&quot;;
</code></pre>
<hr>
<h4 id="12-apache-hive-ddl" tabindex="-1">知识点12：Apache Hive--DDL--建表语句--分桶表的好处、注意事项</h4>
<ul>
<li>分桶表也是一种优化表，可以**<mark>减少join查询时笛卡尔积的数量</mark>**、<mark>提高抽样查询的效率</mark>。</li>
<li>分桶表的字段必须是表中已有的字段；</li>
<li>分桶表需要使用间接的方式才能把数据加载进入：insert+select</li>
<li>在join的时候，针对join的字段进行分桶，可以提高join的效率 减少笛卡尔积数量。</li>
</ul>
<hr>
<h4 id="13-apache-hive-ddl" tabindex="-1">知识点13：Apache Hive--DDL--库、表、分区其他操作</h4>
<blockquote>
<p>因为Hive建表、加载数据及其方便高效；在实际的应用中，<mark>如果建表有问题，通常可以直接drop删除重新创建加载数据</mark>。时间成本极低。</p>
<p>如果表是外部表的话，更加完美了。</p>
</blockquote>
<ul>
<li>Database 数据库 DDL操作</li>
</ul>
<pre><code class="language-sql">--创建数据库
create database if not exists itcast
comment &quot;this is my first db&quot;
with dbproperties ('createdBy'='Allen');

--描述数据库信息
describe database itcast;
describe database extended itcast;
desc database extended itcast;

--切换数据库
use default;
use itcast;
create table t_1(id int);

--删除数据库
--注意 CASCADE关键字慎重使用
DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];
drop database itcast cascade ;


--更改数据库属性
ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);
--更改数据库所有者
ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;
--更改数据库位置
ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path;
</code></pre>
<ul>
<li>Table 表 DDL操作</li>
</ul>
<pre><code class="language-sql">
--下面这两个需要记住
--查询指定表的元数据信息
desc formatted itheima.t_all_hero_part;
show create table t_all_hero_part;

--1、更改表名
ALTER TABLE table_name RENAME TO new_table_name;
--2、更改表属性
ALTER TABLE table_name SET TBLPROPERTIES (property_name = property_value, ... );
--更改表注释
ALTER TABLE student SET TBLPROPERTIES ('comment' = &quot;new comment for student table&quot;);
--3、更改SerDe属性
ALTER TABLE table_name SET SERDE serde_class_name [WITH SERDEPROPERTIES (property_name = property_value, ... )];
ALTER TABLE table_name [PARTITION partition_spec] SET SERDEPROPERTIES serde_properties;
ALTER TABLE table_name SET SERDEPROPERTIES ('field.delim' = ',');
--移除SerDe属性
ALTER TABLE table_name [PARTITION partition_spec] UNSET SERDEPROPERTIES (property_name, ... );

--4、更改表的文件存储格式 该操作仅更改表元数据。现有数据的任何转换都必须在Hive之外进行。
ALTER TABLE table_name  SET FILEFORMAT file_format;
--5、更改表的存储位置路径
ALTER TABLE table_name SET LOCATION &quot;new location&quot;;

--6、更改列名称/类型/位置/注释
CREATE TABLE test_change (a int, b int, c int);
// First change column a's name to a1.
ALTER TABLE test_change CHANGE a a1 INT;
// Next change column a1's name to a2, its data type to string, and put it after column b.
ALTER TABLE test_change CHANGE a1 a2 STRING AFTER b;
// The new table's structure is:  b int, a2 string, c int.
// Then change column c's name to c1, and put it as the first column.
ALTER TABLE test_change CHANGE c c1 INT FIRST;
// The new table's structure is:  c1 int, b int, a2 string.
// Add a comment to column a1
ALTER TABLE test_change CHANGE a1 a1 INT COMMENT 'this is column a1';

--7、添加/替换列
--使用ADD COLUMNS，您可以将新列添加到现有列的末尾但在分区列之前。
--REPLACE COLUMNS 将删除所有现有列，并添加新的列集。
ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type,...);
</code></pre>
<ul>
<li>Partition分区 DDL操作</li>
</ul>
<blockquote>
<p>比较重要的是<mark>增加分区</mark>、<mark>删除分区</mark>操作</p>
</blockquote>
<pre><code class="language-sql">--1、增加分区
--step1: 创建表 手动加载分区数据
drop table if exists t_user_province;
create table t_user_province (
    num int,
    name string,
    sex string,
    age int,
    dept string) partitioned by (province string);

load data local inpath '/root/hivedata/students.txt' into table t_user_province partition(province =&quot;SH&quot;);


--step2:手动创建分区的文件夹 且手动上传文件到分区中 绕开了hive操作  发现hive无法识别新分区
hadoop fs -mkdir /user/hive/warehouse/itheima.db/t_user_province/province=XM
hadoop fs -put students.txt /user/hive/warehouse/itheima.db/t_user_province/province=XM


--step3：修改hive的分区，添加一个分区元数据
ALTER TABLE t_user_province ADD PARTITION (province='XM') location
    '/user/hive/warehouse/itheima.db/t_user_province/province=XM';


----此外还支持一次添加多个分区
ALTER TABLE table_name ADD PARTITION (dt='2008-08-08', country='us') location '/path/to/us/part080808'
    PARTITION (dt='2008-08-09', country='us') location '/path/to/us/part080809';


--2、重命名分区
ALTER TABLE t_user_province PARTITION (province =&quot;SH&quot;) RENAME TO PARTITION (province =&quot;Shanghai&quot;);

--3、删除分区
ALTER TABLE table_name DROP [IF EXISTS] PARTITION (dt='2008-08-08', country='us');
ALTER TABLE table_name DROP [IF EXISTS] PARTITION (dt='2008-08-08', country='us') PURGE; --直接删除数据 不进垃圾桶 有点像skipTrash

--4、修复分区
MSCK [REPAIR] TABLE table_name [ADD/DROP/SYNC PARTITIONS];
--详细使用见课件资料

--5、修改分区
--更改分区文件存储格式
ALTER TABLE table_name PARTITION (dt='2008-08-09') SET FILEFORMAT file_format;
--更改分区位置
ALTER TABLE table_name PARTITION (dt='2008-08-09') SET LOCATION &quot;new location&quot;;
</code></pre>
<h4 id="14-apache-hive-show" tabindex="-1">知识点14：Apache Hive--常见的show语法</h4>
<blockquote>
<p>show databases 数据库</p>
<p>show tables 表</p>
<p>show partitions 表的所有分区 注意必须是分区表才可以执行该语法</p>
<p><mark>desc formatted table_name; 查看表的元数据信息</mark></p>
<p>show create table table_name; 获取表的DDL建表语句</p>
<p>show functions; 函数方法</p>
</blockquote>
<pre><code class="language-sql">--1、显示所有数据库 SCHEMAS和DATABASES的用法 功能一样
show databases;
show schemas;

--2、显示当前数据库所有表/视图/物化视图/分区/索引
show tables;
SHOW TABLES [IN database_name]; --指定某个数据库

--3、显示当前数据库下所有视图
--视图相当于没有数据临时表  虚拟表
Show Views;
SHOW VIEWS 'test_*'; -- show all views that start with &quot;test_&quot;
SHOW VIEWS FROM test1; -- show views from database test1
SHOW VIEWS [IN/FROM database_name];

--4、显示当前数据库下所有物化视图
SHOW MATERIALIZED VIEWS [IN/FROM database_name];

--5、显示表分区信息，分区按字母顺序列出，不是分区表执行该语句会报错
show partitions table_name;
show partitions itheima.student_partition;

--6、显示表/分区的扩展信息
SHOW TABLE EXTENDED [IN|FROM database_name] LIKE table_name;
show table extended like student;
describe formatted itheima.student;

--7、显示表的属性信息
SHOW TBLPROPERTIES table_name;
show tblproperties student;

--8、显示表、视图的创建语句
SHOW CREATE TABLE ([db_name.]table_name|view_name);
show create table student;

--9、显示表中的所有列，包括分区列。
SHOW COLUMNS (FROM|IN) table_name [(FROM|IN) db_name];
show columns  in student;

--10、显示当前支持的所有自定义和内置的函数
show functions;

--11、Describe desc
--查看表信息
desc extended table_name;
--查看表信息（格式化美观）
desc formatted table_name;
--查看数据库相关信息
describe database database_name;
</code></pre>
<hr>
<h4 id="---" tabindex="-1">---</h4>
<h4 id="15-apache-hive-dml-load-local" tabindex="-1">知识点15：Apache Hive--DML--load加载数据（注意local含义）</h4>
<ul>
<li>
<p>功能：load加载操作是将数据文件移动到与 Hive表对 应的位置的<mark>纯复制/移动</mark>操作.</p>
<pre><code class="language-shell"><a class="tag" onclick="toggleTagSearch(this)" data-content="#1、如何理解这个纯字？">#1、如何理解这个纯字？</a>
load命令是单纯数据搬运移到动作，移到加载的过程中，不会对数据进行任何修改操作。

<a class="tag" onclick="toggleTagSearch(this)" data-content="#2、有的是移动操作，有的是复制操作">#2、有的是移动操作，有的是复制操作</a>
</code></pre>
</li>
<li>
<p>语法</p>
<pre><code class="language-sql">LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO
TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
</code></pre>
</li>
<li>
<p>Local关键字的含义</p>
<ul>
<li>local表示是从本地文件系统加载数据到Hive的表中；</li>
<li>如果没有local表示的是从HDFS文件系统加载数据到Hive表中；</li>
<li><mark>问本地指的是哪个本地</mark>？ HS2本地的文件系统</li>
</ul>
</li>
<li>
<p>演示</p>
<pre><code class="language-sql">--step1:建表
--建表student_local 用于演示从本地加载数据
create table student_local(num int,name string,sex string,age int,dept string) row format delimited fields terminated by ',';
--建表student_HDFS  用于演示从HDFS加载数据到非分区表
create table student_HDFS(num int,name string,sex string,age int,dept string) row format delimited fields terminated by ',';
--建表student_HDFS_p 用于演示从HDFS加载数据到分区表
create table student_HDFS_p(num int,name string,sex string,age int,dept string) partitioned by(country string) row format delimited fields terminated by ',';

--step2:加载数据
-- 从本地加载数据  数据位于HS2（node1）本地文件系统  本质是hadoop fs -put上传操作
--Loading data to table itheima.student_local from file:/root/hivedata/students.txt
LOAD DATA LOCAL INPATH '/root/hivedata/students.txt' INTO TABLE student_local;

--从HDFS加载数据  数据位于HDFS文件系统根目录下  本质是hadoop fs -mv 移动操作
--Loading data to table itheima.student_hdfs from hdfs://node1:8020/stu/students.txt

--先把数据上传到HDFS上  hadoop fs -put /root/hivedata/students.txt /
LOAD DATA INPATH '/students.txt' INTO TABLE student_HDFS;

----从HDFS加载数据到分区表中并指定分区  数据位于HDFS文件系统根目录下
--先把数据上传到HDFS上 hadoop fs -put /root/hivedata/students.txt /
LOAD DATA INPATH '/students.txt' INTO TABLE student_HDFS_p partition(country =&quot;China&quot;);
</code></pre>
</li>
<li>
<p>总结：<mark>建议在HIve中使用load命令加载数据到表中</mark>，这也是Hive比Mysql加载数据高效地方所在。</p>
</li>
<li>
<p>其实不管是使用load还是直接使用hadoop fs,目标就是把文件放置在表对应的目录下。</p>
</li>
</ul>
<hr>
<h4 id="16-apache-hive-dml-insert" tabindex="-1">知识点16：Apache Hive--DML--insert插入语法</h4>
<ul>
<li>
<p>探讨</p>
<pre><code class="language-sql">--如果使用操作Mysql的思维来玩Hive会如何  使用insert+values的方式插入数据。

create table t_insert(id int,name string);

insert into table t_insert values(1,&quot;allen&quot;);

--可以执行 但是效率极低 因为底层是通过MapReduce插入数据的  因此实际中推荐使用load加载数据
</code></pre>
</li>
</ul>
<p><strong><mark>insert+select</mark></strong></p>
<blockquote>
<p><mark>在hive中，insert主要是结合 select 查询语句使用，将查询结果插入到表中</mark>。</p>
</blockquote>
<ul>
<li>保证后面select<strong>查询语句返回的结果字段个数、类型、顺序和待插入表一致</strong>；</li>
<li>如果不一致，Hive会尝试帮你转换，但是不保证成功；</li>
<li>insert+select也是在数仓中ETL数据常见的操作。</li>
</ul>
<pre><code class="language-sql">--step1:创建一张源表student
drop table if exists student;
create table student(num int,name string,sex string,age int,dept string)
row format delimited
fields terminated by ',';
--加载数据
load data local inpath '/root/hivedata/students.txt' into table student;

select * from student;

--step2：创建一张目标表  只有两个字段
create table student_from_insert(sno int,sname string);
--使用insert+select插入数据到新表中
insert into table student_from_insert
select num,name from student;

select *
from student_from_insert;
</code></pre>
<p>Multi Inserts <mark>多重插入</mark></p>
<ul>
<li>
<p>功能：<mark><strong>一次扫描，多次插入</strong></mark></p>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">create table source_table (id int, name string) row format delimited fields terminated by ',';

create table test_insert1 (id int) row format delimited fields terminated by ',';
create table test_insert2 (name string) row format delimited fields terminated by ',';


--普通插入：
insert into table test_insert1 select id from source_table;
insert into table test_insert2 select name from source_table;

--在上述需求实现中 从同一张表扫描了2次 分别插入不同的目标表中 性能低下。

--多重插入：
from source_table                     
insert overwrite table test_insert1 
select id
insert overwrite table test_insert2
select name;
--只需要扫描一次表  分别把不同字段插入到不同的表中即可 减少扫描次数 提高效率
</code></pre>
</li>
</ul>
<p>Dynamic partition inserts <mark>动态分区插入</mark></p>
<ul>
<li>
<p>何谓动态分区，静态分区</p>
<pre><code class="language-sql">针对的是分区表。
--问题：分区表中分区字段值是如何确定的？

1、如果是在加载数据的时候人手动写死指定的  叫做静态分区 
load data local inpath '/root/hivedata/usa_dezhou.txt'  into table t_user_double_p partition(guojia=&quot;meiguo&quot;,sheng=&quot;dezhou&quot;);

2、如果是通过insert+select 动态确定分区值的，叫做动态分区
insert table partition (分区字段) +select 
</code></pre>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">--1、首先设置动态分区模式为非严格模式 默认已经开启了动态分区功能
set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;

--2、当前库下已有一张表student
select * from student;

--3、创建分区表 以sdept作为分区字段
create table student_partition(Sno int,Sname string,Sex string,Sage int) partitioned by(Sdept string);

--4、执行动态分区插入操作
insert into table student_partition partition(Sdept)
select num,name,sex,age,dept from student;
--其中，num,name,sex,age作为表的字段内容插入表中
--dept作为分区字段值

select *
from student_partition;

show partitions student_partition;
</code></pre>
</li>
</ul>
<hr>
<h4 id="17-apache-hive-dml-insert" tabindex="-1">知识点17：Apache Hive--DML--insert导出数据操作</h4>
<ul>
<li>
<p>功能：把select查询的结果导出成为一个文件。</p>
</li>
<li>
<p>注意：<strong><mark>导出操作是一个overwrite操作</mark></strong>，可能会让你凉凉。慎重！！！！</p>
<ul>
<li>致命栗子：<code>insert overwrite directory '/' select num,name,age from student_hdfs limit 2;</code>：<strong>根目录文件全部会被删掉</strong>=<code>rm -rf /*</code></li>
</ul>
</li>
<li>
<p>语法</p>
<pre><code class="language-sql">--当前库下已有一张表student
select * from student_hdfs;

--1、导出查询结果到HDFS指定目录下
insert overwrite directory '/tmp/hive_export/e1' select num,name,age from student_hdfs limit 2;   --默认导出数据字段之间的分隔符是\001

--2、导出时指定分隔符和文件存储格式
insert overwrite directory '/tmp/hive_export/e2' row format delimited fields terminated by ','
stored as orc
select num,name,age from student_hdfs limit 2;

--3、导出数据到本地文件系统指定目录下
insert overwrite local directory '/root/hive_export/e1' select num,name,age from student_hdfs limit 2;
</code></pre>
</li>
</ul>
<hr>
<h4 id="今日课程作业" tabindex="-1">今日课程作业</h4>
<pre><code class="language-shell"><a class="tag" onclick="toggleTagSearch(this)" data-content="#1、Hive">#1、Hive</a> DDL 建表语句  重点之重点
	数据类型
	SerDe序列化机制（Hive怎么读写HDFS文件的）
	分隔符指定
	默认分隔符
	内外部表
	存储路径location
	
	分区表
	分桶表
		分区、分桶是什么
		解决了什么问题
		如何创建
		如何加载
		如何使用
		注意事项
<a class="tag" onclick="toggleTagSearch(this)" data-content="#2、Hive">#2、Hive</a> DDL Alter
	partition的增加、修改、修复

<a class="tag" onclick="toggleTagSearch(this)" data-content="#3、常见的show">#3、常见的show</a>
	库  
	表
	分区
	建表语句
	表元数据信息
	函数方法
    
<a class="tag" onclick="toggleTagSearch(this)" data-content="#4、hive">#4、hive</a> DML  重点之重点
	load
		local本地是哪里？
		如何理解load是一个纯操作？纯是什么意思？
		什么时候是复制  什么时候是加载
	insert
		insert+select
		多重插入  优化
		动态分区插入   <a class="tag" onclick="toggleTagSearch(this)" data-content="#这个很重要">#这个很重要</a>
			什么叫动态 静态
			动态分区的严格模式  非严格模式是啥
		数据导出操作
			注意overwrite覆盖
</code></pre>
</main>
<aside>
<div class="sidebar">
<div class="sidebar-container">
<div class="toc">
<div class="toc-title-container">
<div class="toc-title">
On this page
</div>
</div>
<div class="toc-container">
<nav class="toc">
<ol>
<li><a href="#hadoop-day06-apache-hive-sql-ddl-dml">hadoop离线day06-Apache Hive SQL-DDL、DML</a>
<ol>
<li><a href="#今日课程学习目标">今日课程学习目标</a>
</li>
<li><a href="#今日课程内容大纲">今日课程内容大纲</a>
</li>
<li><a href="#01-apache-hive-ddl">知识点01：Apache Hive--DDL--概念与语法树介绍</a>
</li>
<li><a href="#02-apache-hive-ddl">知识点02：Apache Hive--DDL--建表语句--表存在忽略异常</a>
</li>
<li><a href="#03-apache-hive-ddl">知识点03：Apache Hive--DDL--建表语句--数据类型</a>
</li>
<li><a href="#04-apache-hive-ddl-ser-de">知识点04：Apache Hive--DDL--建表语句--SerDe机制、分隔符指定语法</a>
</li>
<li><a href="#05-apache-hive-ddl">知识点05：Apache Hive--DDL--建表语句--默认分隔符</a>
</li>
<li><a href="#06-apache-hive-ddl">知识点06：Apache Hive--DDL--建表语句--内部表、外部表</a>
</li>
<li><a href="#07-apache-hive-ddl-location">知识点07：Apache Hive--DDL--建表语句--location存储位置</a>
</li>
<li><a href="#08-apache-hive-ddl">知识点08：Apache Hive--DDL--建表语句--分区表创建、静态数据加载、分区裁剪</a>
</li>
<li><a href="#09-apache-hive-ddl">知识点09：Apache Hive--DDL--建表语句--动态分区插入数据</a>
</li>
<li><a href="#10-apache-hive-ddl">知识点10：Apache Hive--DDL--建表语句--多重分区及分区表注意事项</a>
</li>
<li><a href="#11-apache-hive-ddl">知识点11：Apache Hive--DDL--建表语句--分桶表语法、创建、加载</a>
</li>
<li><a href="#12-apache-hive-ddl">知识点12：Apache Hive--DDL--建表语句--分桶表的好处、注意事项</a>
</li>
<li><a href="#13-apache-hive-ddl">知识点13：Apache Hive--DDL--库、表、分区其他操作</a>
</li>
<li><a href="#14-apache-hive-show">知识点14：Apache Hive--常见的show语法</a>
</li>
<li><a href="#---">---</a>
</li>
<li><a href="#15-apache-hive-dml-load-local">知识点15：Apache Hive--DML--load加载数据（注意local含义）</a>
</li>
<li><a href="#16-apache-hive-dml-insert">知识点16：Apache Hive--DML--insert插入语法</a>
</li>
<li><a href="#17-apache-hive-dml-insert">知识点17：Apache Hive--DML--insert导出数据操作</a>
</li>
<li><a href="#今日课程作业">今日课程作业</a>
</li>
</ol>
</li>
</ol>
</nav>
</div>
</div>
<div class="backlinks">
<div class="backlink-title" style="margin:4px 0!important">Pages mentioning this page</div>
<div class="backlink-list"><div class="backlink-card"><i icon-name="link"></i><a href="/czc知识库/计算机/Hadoop技术栈/Hadoop技术栈/" data-note-icon="" class="backlink">Hadoop技术栈</a>
</div></div>
</div>
</div>
</div>
</aside>
<style>#tooltip-wrapper{background:var(--background-primary);padding:1em;border-radius:4px;overflow:hidden;position:fixed;width:80%;max-width:400px;height:auto;max-height:300px;font-size:.8em;box-shadow:0 5px 10px rgba(0,0,0,.1);opacity:0;transition:opacity .1s;unicode-bidi:plaintext;overflow-y:scroll;z-index:10}#tooltip-wrapper:after{content:"";position:absolute;z-index:1;bottom:0;left:0;pointer-events:none;width:100%;unicode-bidi:plaintext;height:75px}</style>
<div style="opacity:0;display:none" id="tooltip-wrapper">
<div id="tooltip-content">
</div>
</div>
<iframe style="display:none;height:0;width:0" id="link-preview-iframe" src="">
</iframe>
<script>var opacityTimeout,contentTimeout,transitionDurationMs=100,iframe=document.getElementById("link-preview-iframe"),tooltipWrapper=document.getElementById("tooltip-wrapper"),tooltipContent=document.getElementById("tooltip-content"),linkHistories={};function hideTooltip(){opacityTimeout=setTimeout((function(){tooltipWrapper.style.opacity=0,contentTimeout=setTimeout((function(){tooltipContent.innerHTML="",tooltipWrapper.style.display="none"}),transitionDurationMs+1)}),transitionDurationMs)}function showTooltip(t){var e=t.target,o=e.getClientRects()[e.getClientRects().length-1],i=window.pageYOffset||document.documentElement.scrollTop,n=t.target.getAttribute("href");if(-1===n.indexOf("http")||-1!==n.indexOf(window.location.host)){let t=n.split("#")[0];linkHistories[t]?(tooltipContent.innerHTML=linkHistories[t],tooltipWrapper.style.display="block",setTimeout((function(){if(tooltipWrapper.style.opacity=1,-1!=n.indexOf("#")){let t=n.split("#")[1];const e=tooltipWrapper.querySelector(`[id='${t}']`);e.classList.add("referred"),e.scrollIntoView({behavior:"smooth"},!0)}else tooltipWrapper.scroll(0,0)}),1)):(iframe.src=t,iframe.onload=function(){tooltipContentHtml="",tooltipContentHtml+='<div style="font-weight: bold; unicode-bidi: plaintext;">'+iframe.contentWindow.document.querySelector("h1").innerHTML+"</div>",tooltipContentHtml+=iframe.contentWindow.document.querySelector(".content").innerHTML,tooltipContent.innerHTML=tooltipContentHtml,linkHistories[t]=tooltipContentHtml,tooltipWrapper.style.display="block",tooltipWrapper.scrollTop=0,setTimeout((function(){if(tooltipWrapper.style.opacity=1,-1!=n.indexOf("#")){let t=n.split("#")[1];const e=tooltipWrapper.querySelector(`[id='${t}']`);e.classList.add("referred"),console.log(e),e.scrollIntoView({behavior:"smooth"},!0)}else tooltipWrapper.scroll(0,0)}),1)}),tooltipWrapper.style.left=o.left-tooltipWrapper.offsetWidth/2+o.width/2+"px",window.innerHeight-o.top<tooltipWrapper.offsetHeight?tooltipWrapper.style.top=o.top+i-tooltipWrapper.offsetHeight-10+"px":window.innerHeight-o.top>tooltipWrapper.offsetHeight&&(tooltipWrapper.style.top=o.top+i+35+"px"),o.left+o.width/2<tooltipWrapper.offsetWidth/2?tooltipWrapper.style.left="10px":document.body.clientWidth-o.left-o.width/2<tooltipWrapper.offsetWidth/2&&(tooltipWrapper.style.left=document.body.clientWidth-tooltipWrapper.offsetWidth-20+"px")}}function setupListeners(t){t.addEventListener("mouseleave",(function(t){hideTooltip()})),tooltipWrapper.addEventListener("mouseleave",(function(t){hideTooltip()})),t.addEventListener("mouseenter",(function(t){clearTimeout(opacityTimeout),clearTimeout(contentTimeout),showTooltip(t)})),tooltipWrapper.addEventListener("mouseenter",(function(t){clearTimeout(opacityTimeout),clearTimeout(contentTimeout)}))}window.addEventListener("load",(function(t){document.querySelectorAll(".internal-link").forEach(setupListeners),document.querySelectorAll(".backlink-card a").forEach(setupListeners)}))</script>
<script>window.location.hash&&document.getElementById(window.location.hash.slice(1)).classList.add("referred"),window.addEventListener("hashchange",(e=>{const t=e.oldURL.split("#");t[1]&&document.getElementById(t[1]).classList.remove("referred");const n=e.newURL.split("#");n[1]&&document.getElementById(n[1]).classList.add("referred")}),!1);const url_parts=window.location.href.split("#"),url=url_parts[0],referrence=url_parts[1];document.querySelectorAll(".cm-s-obsidian > *[id]").forEach((function(e){e.ondblclick=function(e){const t=url+"#"+e.target.id;navigator.clipboard.writeText(t)}}))</script>
<script src="https://fastly.jsdelivr.net/npm/luxon@3.2.1/build/global/luxon.min.js"></script>
<script defer="defer">TIMESTAMP_FORMAT="MMM dd, yyyy h:mm a",document.querySelectorAll(".human-date").forEach((function(e){date=e.getAttribute("data-date")||e.innerText,parsed_date=luxon.DateTime.fromISO(date),null!=parsed_date.invalid&&(parsed_date=luxon.DateTime.fromSQL(date)),null!=parsed_date.invalid&&(parsed_date=luxon.DateTime.fromHTML(date)),e.innerHTML=parsed_date.toFormat(TIMESTAMP_FORMAT)}))</script>
<script>lucide.createIcons({attrs:{class:["svg-icon"]}})</script>
</body>
</html>
