<!doctype html>
<html lang="zh-CN">
<head>
<title>8-Apache Hive函数高阶应用、性能调优</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<script async type="module">import mermaid from"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs"</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js" integrity="sha512-hpZ5pDCF2bRCweL5WoA0/N1elet1KYL5mx3LP555Eg/0ZguaHawxNvEjF6O3rufAChs16HVNhEc6blF/rZoowQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-sv0slik/5O0JIPdLBCR2A3XDg/1U3WuDEheZfI/DI5n8Yqc3h5kjrnr46FGBNiUAJF7rE4LHKwQ/SoSLRKAxEA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script async src="https://cdn.jsdelivr.net/npm/lucide@0.115.0/dist/umd/lucide.min.js"></script>
<script>window.addEventListener("load",(()=>{document.querySelectorAll(".callout").forEach((e=>{const t=getComputedStyle(e).getPropertyValue("--callout-icon"),l=t&&t.trim().replace(/^lucide-/,"");if(l){const t=e.querySelector(".callout-title");if(t){const e=document.createElement("div"),c=document.createElement("i");e.appendChild(c),c.setAttribute("icon-name",l),e.setAttribute("class","callout-icon"),t.insertBefore(e,t.firstChild)}}})),lucide.createIcons(),Array.from(document.querySelectorAll(".callout.is-collapsible")).forEach((e=>{e.querySelector(".callout-title").addEventListener("click",(t=>{e.classList.contains("is-collapsed")?e.classList.remove("is-collapsed"):e.classList.add("is-collapsed")}))}))}))</script>
<script async src="https://fastly.jsdelivr.net/npm/force-graph@1.43.0/dist/force-graph.min.js"></script>
<script async src="https://fastly.jsdelivr.net/npm/@alpinejs/persist@3.11.1/dist/cdn.min.js"></script>
<script src="https://fastly.jsdelivr.net/npm/alpinejs@3.11.1/dist/cdn.min.js" async></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism-okaidia.min.css" integrity="sha512-mIs9kKbaw6JZFfSuo+MovjU+Ntggfoj8RwAmJbVXQ5mkAX5LlgETQEweFPI18humSPHymTb5iikEOKWF7I8ncQ==" crossorigin="anonymous" referrerpolicy="no-referrer" async>
<script src="https://fastly.jsdelivr.net/npm/whatwg-fetch@3.6.2/dist/fetch.umd.min.js" crossorigin="anonymous" referrerpolicy="no-referrer" async></script>
<link href="/styles/digital-garden-base.css" rel="stylesheet">
<link href="/styles/obsidian-base.css" rel="stylesheet">
<link href="/styles/_theme.74516f71.css" rel="stylesheet">
<link href="/styles/custom-style.css" rel="stylesheet">
<link rel="icon" href="/favicon.ico" sizes="any">
<link rel="icon" href="/favicon.svg" type="image/svg+xml">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="manifest" href="/manifest.webmanifest">
<style></style>
<style></style>
</head>
<body class="theme-light markdown-preview-view markdown-rendered markdown-preview-section css-settings-manager mod-windows is-frameless is-maximized is-hidden-frameless is-focused obsidian-app theme-light show-inline-title show-ribbon show-view-header css-settings-manager theme-default line-style-solid folder-default blockquote-normal callout-normal checkbox-default tag-default link-default heading-default responsive-tile-height oz-show-all-num">
<nav class="navbar">
<div class="navbar-inner">
<a href="/" style="text-decoration:none">
<h1 style="margin:15px!important">czc&#39;s digital garden</h1>
</a>
</div>
<div class="search-button align-icon" onclick="toggleSearch()">
<span class="search-icon">
<i icon-name="search"></i>
</span>
<span class="search-text">
<span>Search</span>
<span style="font-size:.6rem;padding:2px 2px 0 6px;text-align:center;transform:translateY(4px)" class="search-keys">
CTRL + K
</span>
</span>
</div>
</nav>
<div class="search-container" id="globalsearch" onclick="toggleSearch()">
<div class="search-box">
<input type="search" id="term" placeholder="Start typing...">
<div id="search-results"></div>
<footer class="search-box-footer">
<div class="navigation-hint">
<span>Enter to select</span>
</div>
<div class="navigation-hint align-icon">
<i icon-name="arrow-up" aria-hidden="true"></i>
<i icon-name="arrow-down" aria-hidden="true"></i>
<span>to navigate</span>
</div>
<div class="navigation-hint">
<span>ESC to close</span>
</div>
</footer>
</div>
</div>
<script src="https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js"></script>
<script>document.addEventListener("DOMContentLoaded",init,!1),document.addEventListener("DOMContentLoaded",setCorrectShortcut,!1),window.toggleSearch=function(){document.getElementById("globalsearch").classList.contains("active")?document.getElementById("globalsearch").classList.remove("active"):(document.getElementById("globalsearch").classList.add("active"),document.getElementById("term").focus())},window.toggleTagSearch=function(e){console.log(e.textContent);const t=e.textContent;t&&(window.document.getElementById("term").value=t.trim(),window.toggleSearch(),window.search())};const loadingSvg='\n    <svg width="100" height="100" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg" stroke="#fff">\n      <g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2">\n          <circle cx="22" cy="22" r="6" stroke-opacity="0">\n              <animate attributeName="r"\n                   begin="1.5s" dur="3s"\n                   values="6;22"\n                   calcMode="linear"\n                   repeatCount="indefinite" />\n              <animate attributeName="stroke-opacity"\n                   begin="1.5s" dur="3s"\n                   values="1;0" calcMode="linear"\n                   repeatCount="indefinite" />\n              <animate attributeName="stroke-width"\n                   begin="1.5s" dur="3s"\n                   values="2;0" calcMode="linear"\n                   repeatCount="indefinite" />\n          </circle>\n          <circle cx="22" cy="22" r="6" stroke-opacity="0">\n              <animate attributeName="r"\n                   begin="3s" dur="3s"\n                   values="6;22"\n                   calcMode="linear"\n                   repeatCount="indefinite" />\n              <animate attributeName="stroke-opacity"\n                   begin="3s" dur="3s"\n                   values="1;0" calcMode="linear"\n                   repeatCount="indefinite" />\n              <animate attributeName="stroke-width"\n                   begin="3s" dur="3s"\n                   values="2;0" calcMode="linear"\n                   repeatCount="indefinite" />\n          </circle>\n          <circle cx="22" cy="22" r="8">\n              <animate attributeName="r"\n                   begin="0s" dur="1.5s"\n                   values="6;1;2;3;4;5;6"\n                   calcMode="linear"\n                   repeatCount="indefinite" />\n          </circle>\n      </g>\n  </svg>';function debounce(e,t,n){var a;return function(){var r=this,i=arguments,c=n&&!a;clearTimeout(a),a=setTimeout((function(){a=null,n||e.apply(r,i)}),t),c&&e.apply(r,i)}}function setCorrectShortcut(){navigator.platform.toUpperCase().indexOf("MAC")>=0&&document.querySelectorAll(".search-keys").forEach((e=>e.innerHTML="⌘ + K"))}function createIndex(e){const t=e=>e.toLowerCase().split(/([^a-z]|[^\x00-\x7F])/),n=new FlexSearch.Document({cache:!0,charset:"latin:extra",optimize:!0,index:[{field:"content",tokenize:"reverse",encode:t},{field:"title",tokenize:"forward",encode:t},{field:"tags",tokenize:"forward",encode:t}]});return e.forEach(((e,t)=>{n.add({id:t,title:e.title,content:e.content,tags:e.tags})})),n}async function init(){let e=!0;if(localStorage.getItem("searchIndex")){let{date:t,docs:n}=JSON.parse(localStorage.getItem("searchIndex"));if("2025-06-29T04:38:31.215Z"===t){e=!1;let t=createIndex(n);window.docs=n,window.index=t}}if(e){let e=await(await fetch("/searchIndex.json?v=2025-06-29T04:38:31.215Z")).json(),t=createIndex(e);localStorage.setItem("searchIndex",JSON.stringify({date:"2025-06-29T04:38:31.215Z",docs:e})),window.docs=e,window.index=t}document.addEventListener("keydown",(e=>{if((e.ctrlKey||e.metaKey)&&"k"===e.key&&(e.preventDefault(),toggleSearch()),"Escape"===e.key&&document.getElementById("globalsearch").classList.remove("active"),document.getElementById("globalsearch").classList.contains("active")){if("ArrowDown"===e.key){e.preventDefault();let t=document.querySelector(".searchresult.active");t?(t.classList.remove("active"),t.nextElementSibling?t.nextElementSibling.classList.add("active"):document.querySelector(".searchresult").classList.add("active")):document.querySelector(".searchresult").classList.add("active");let n=document.querySelector(".searchresult.active");n&&n.scrollIntoView({behavior:"smooth",block:"nearest",inline:"start"})}if("ArrowUp"===e.key){e.preventDefault();let t=document.querySelector(".searchresult.active");t?(t.classList.remove("active"),t.previousElementSibling?t.previousElementSibling.classList.add("active"):document.querySelectorAll(".searchresult").forEach((e=>{e.nextElementSibling||e.classList.add("active")}))):document.querySelectorAll(".searchresult").forEach((e=>{e.nextElementSibling&&e.classList.add("active")}));let n=document.querySelector(".searchresult.active");n&&n.scrollIntoView({behavior:"smooth",block:"nearest",inline:"start"})}if("Enter"===e.key){e.preventDefault();let t=document.querySelector(".searchresult.active");t&&(window.location.href=t.querySelector("a").href)}}}));const t=debounce(search,200,!1);field=document.querySelector("#term"),field.addEventListener("keydown",(e=>{"ArrowDown"!==e.key&&"ArrowUp"!==e.key&&t()})),resultsDiv=document.querySelector("#search-results");const n=new URL(location.href).searchParams;n.get("q")&&(field.setAttribute("value",n.get("q")),toggleSearch(),search())}async function search(){let e=field.value.trim();if(!e)return;if(e==lastSearch)return;console.log(`search for ${e}`),window.lastSearch=e,resultsDiv.innerHTML=loadingSvg;let t=offlineSearch(e),n="";if(!t.length){let t=document.createElement("p");return t.innerText=`No results for "${e}"`,resultsDiv.innerHTML="",void resultsDiv.appendChild(t)}n+='<div style="max-width:100%;">',t.forEach((e=>{e.tags&&e.tags.length>0?n+=`<div class="searchresult">\n                    <a class="search-link" href="${e.url}">${e.title}</a>\n                    <div onclick="window.location='${e.url}'">\n                        <div class="header-meta">\n                            <div class="header-tags">\n                                ${e.tags.map((e=>'<a class="tag" href="JavaScript:Void(0);">#'+e+"</a>")).join("")}\n                            </div>\n                        </div>\n                        ${e.content}\n                    </div>\n                </div>`:n+=`<div class="searchresult">\n                    <a class="search-link" href="${e.url}">${e.title}</a>\n                    <div onclick="window.location='${e.url}'">\n                        ${e.content}\n                    </div>\n                </div>`})),n+="</div>",resultsDiv.innerHTML=n}function truncate(e,t){return(e=e.replaceAll(/<[^>]*>/g,"")).length<t?e:e.substring(0,t-3)+"..."}function offlineSearch(e){let t=window.docs,n="#"===e[0]&&e.length>1?index.search(e.substring(1),[{field:"tags"}]):index.search(e,[{field:"title",limit:5},{field:"content",weight:10}]);const a=e=>{const t=n.filter((t=>t.field===e));return 0===t.length?[]:[...t[0].result]};return[...new Set([...a("title"),...a("content"),...a("tags")])].map((e=>{let n=t[e];return n.content=truncate(n.content,400),n.tags=n.tags.filter((e=>"gardenEntry"!=e&&"note"!=e)),n}))}window.lastSearch=""</script>
<main class="content cm-s-obsidian">
<header>
<h1 data-note-icon="">8-Apache Hive函数高阶应用、性能调优</h1>
<div class="header-meta">
<div class="header-tags">
</div>
<div class="timestamps"><div><i icon-name="calendar-plus"></i> <span class="human-date" data-date="2025-06-09T10:30:37.807+08:00"></span></div><div><i icon-name="calendar-clock"></i> <span class="human-date" data-date="2025-04-12T18:51:51.000+08:00"></span></div></div></div>
</header>
<h3 id="hadoop-day08-apache-hive" tabindex="-1">hadoop离线day08-Apache Hive函数高阶应用、性能调优</h3>
<hr>
<h4 id="今日课程学习目标" tabindex="-1">今日课程学习目标</h4>
<pre><code class="language-shell">掌握explode函数、侧视图使用
掌握行列转换、json数据处理
掌握窗口函数的使用
知道Hive数据压缩、文件存储格式
掌握Hive通用调优（重要的见下述大纲）
</code></pre>
<h4 id="今日课程内容大纲" tabindex="-1">今日课程内容大纲</h4>
<pre><code class="language-shell"><a class="tag" onclick="toggleTagSearch(this)" data-content="#Hive函数高阶应用（面试笔试、开发高频区域）">#Hive函数高阶应用（面试笔试、开发高频区域）</a>
	explode(UDTF)函数功能
	lateral view 侧视图
	行列转换
	json格式数据解析
	窗口函数（Window function）开窗函数
		分组TopN、级联累加问题、连续登陆
<a class="tag" onclick="toggleTagSearch(this)" data-content="#Hive的性能调优">#Hive的性能调优</a>
	hive的数据文件格式  数据压缩
		行式存储 列式存储（ORC parquet）
	hive通用调优	
		*join优化
		*group by数据倾斜优化
		*task并行度问题
		其他通用调优	
</code></pre>
<hr>
<h4 id="01-apache-hive-explode-udtf" tabindex="-1">知识点01：Apache Hive--explode函数的使用与限制（UDTF表生成函数）</h4>
<ul>
<li>
<p><mark>explode属于UDTF函数</mark>，表生成函数，输入一行数据输出多行数据。</p>
</li>
<li>
<p>功能：</p>
<pre><code class="language-sql">explode() takes in an array (or a map) as an input and outputs the elements of the array (map) as separate rows.

--explode接收map array类型的参数 把map或者array的元素输出，一行一个元素。

explode(array(11,22,33))         11
	                             22
	                             33
	                             
	                             
select explode(`array`(11,22,33,44,55));
select explode(`map`(&quot;id&quot;,10086,&quot;name&quot;,&quot;allen&quot;,&quot;age&quot;,18));	                             
</code></pre>
</li>
<li>
<p>栗子</p>
<blockquote>
<p>将NBA总冠军球队数据使用explode进行拆分，并且根据夺冠年份进行倒序排序。</p>
</blockquote>
<pre><code class="language-sql">--step1:建表
create table the_nba_championship(
           team_name string,
           champion_year array&lt;string&gt;
) row format delimited
fields terminated by ','
collection items terminated by '|';

--step2:加载数据文件到表中
load data local inpath '/root/hivedata/The_NBA_Championship.txt' into table the_nba_championship;

--step3:验证
select * from the_nba_championship;

--step4:使用explode函数对champion_year进行拆分 俗称炸开
select explode(champion_year) from the_nba_championship;

--想法是正确的 sql执行确实错误的
select team_name,explode(champion_year) from the_nba_championship;
--错误信息
UDTF’s are not supported outside the SELECT clause, nor nested in expressions
UDTF 在 SELECT 子句之外不受支持，也不在表达式中嵌套？？？

</code></pre>
</li>
<li>
<p>如果数据不是map或者array 如何使用explode函数呢？</p>
<blockquote>
<p>想法设法使用split subsrt regex_replace等函数组合使用 把数据变成array或者map.</p>
</blockquote>
<pre><code class="language-sql">create table the_nba_championship_str(
           team_name string,
           champion_year string
) row format delimited
fields terminated by ',';

load data local inpath '/root/hivedata/The_NBA_Championship.txt' into table the_nba_championship_str;
</code></pre>
</li>
</ul>
<hr>
<h4 id="02-apache-hive-lateral-view" tabindex="-1">知识点02：Apache Hive--lateral view侧视图的使用</h4>
<blockquote>
<p>侧视图的原理是<mark>将UDTF的结果构建成一个类似于视图的表，然后将原表中的每一行和UDTF函数输出的每一行进行连接，生成一张新的虚拟表</mark></p>
</blockquote>
<ul>
<li>
<p>背景</p>
<ul>
<li>
<p>UDTF函数生成的结果可以当成一张虚拟的表，但是无法和原始表进行组合查询</p>
<pre><code class="language-sql">select name,explode(location) from test_message;
--这个sql就是错误的  相当于执行组合查询 
</code></pre>
</li>
<li>
<p>从理论层面推导，对两份数据进行join就可以了</p>
</li>
<li>
<p>但是，hive专门推出了lateral view侧视图的语，满足上述需要。</p>
</li>
</ul>
</li>
<li>
<p>功能：<mark>把UDTF函数生成的结果和原始表进行关联，便于用户在select时间组合查询</mark>、 lateral view是UDTf的好基友好搭档，实际中经常配合使用。</p>
</li>
<li>
<p>语法：</p>
<pre><code class="language-sql">--lateral view侧视图基本语法如下
select …… from tabelA lateral view UDTF(xxx) 别名 as col1,col2,col3……;

--针对上述NBA冠军球队年份排名案例，使用explode函数+lateral view侧视图，可以完美解决
select a.team_name ,b.year
from the_nba_championship a lateral view explode(champion_year) b as year;

--根据年份倒序排序
select a.team_name ,b.year
from the_nba_championship a lateral view explode(champion_year) b as year
order by b.year desc;

--统计每个球队获取总冠军的次数 并且根据倒序排序
select a.team_name ,count(*) as nums
from the_nba_championship a lateral view explode(champion_year) b as year
group by a.team_name
order by nums desc;
</code></pre>
</li>
</ul>
<hr>
<h4 id="03-apache-hive-collect-list-concat-ws" tabindex="-1">知识点03：Apache Hive--行列转换--多行转单列（collect_list、concat_ws）</h4>
<ul>
<li>
<p><mark><strong>数据收集函数</strong></mark></p>
<pre><code class="language-sql">collect_set --把多行数据收集为一行  返回set集合  去重无序
collect_list --把多行数据收集为一行  返回list集合  不去重有序
</code></pre>
</li>
<li>
<p>字符串拼接函数</p>
<pre><code class="language-sql">concat  --直接拼接字符串
concat_ws --指定分隔符拼接

select concat(&quot;it&quot;,&quot;cast&quot;,&quot;And&quot;,&quot;heima&quot;);
select concat(&quot;it&quot;,&quot;cast&quot;,&quot;And&quot;,null);

select concat_ws(&quot;-&quot;,&quot;itcast&quot;,&quot;And&quot;,&quot;heima&quot;);
select concat_ws(&quot;-&quot;,&quot;itcast&quot;,&quot;And&quot;,null);
</code></pre>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">--原表
+----------------+----------------+----------------+--+
| row2col2.col1  | row2col2.col2  | row2col2.col3  |
+----------------+----------------+----------------+--+
| a              | b              | 1              |
| a              | b              | 2              |
| a              | b              | 3              |
| c              | d              | 4              |
| c              | d              | 5              |
| c              | d              | 6              |
+----------------+----------------+----------------+--+

--目标表
+-------+-------+--------+--+
| col1  | col2  |  col3  |
+-------+-------+--------+--+
| a     | b     | 1-2-3  |
| c     | d     | 4-5-6  |
+-------+-------+--------+--+

--建表
create table row2col2(
                         col1 string,
                         col2 string,
                         col3 int
)row format delimited fields terminated by '\t';

--加载数据到表中
load data local inpath '/root/hivedata/r2c2.txt' into table row2col2;
select * from row2col2;

--最终SQL实现
select
    col1,
    col2,
    concat_ws(',', collect_list(cast(col3 as string))) as col3
from
    row2col2
group by
    col1, col2;
</code></pre>
</li>
</ul>
<hr>
<h4 id="04-apache-hive-explode-lateral-view" tabindex="-1">知识点04：Apache Hive--行列转换--单列转多行（explode、lateral view）</h4>
<ul>
<li>
<p>技术原理： explode+lateral view</p>
</li>
<li>
<p>例子</p>
<pre><code class="language-sql">--原表
+-------+-------+--------+--+
| col1  | col2  |  col3  |
+-------+-------+--------+--+
| a     | b     | 1,2,3  |
| c     | d     | 4,5,6  |
+-------+-------+--------+--+

--目标表
+----------------+----------------+----------------+--+
| row2col2.col1  | row2col2.col2  | row2col2.col3  |
+----------------+----------------+----------------+--+
| a              | b              | 1              |
| a              | b              | 2              |
| a              | b              | 3              |
| c              | d              | 4              |
| c              | d              | 5              |
| c              | d              | 6              |
+----------------+----------------+----------------+--+

--创建表
create table col2row2(
                         col1 string,
                         col2 string,
                         col3 string
)row format delimited fields terminated by '\t';

--加载数据
load data local inpath '/root/hivedata/c2r2.txt' into table col2row2;

select * from col2row2;

select explode(split(col3,',')) from col2row2;

--SQL最终实现
select
    col1,
    col2,
    lv.col3 as col3
from
    col2row2
        lateral view
            explode(split(col3, ',')) lv as col3;
</code></pre>
</li>
</ul>
<hr>
<h4 id="05-apache-hive-json" tabindex="-1">知识点05：Apache Hive--json格式数据处理</h4>
<ul>
<li>
<p>在hive中，没有json类的存在，一般使<mark>用string类型来修饰</mark>，叫做json字符串，简称<mark>json串</mark>。</p>
</li>
<li>
<p>在hive中，处理json数据的两种方式</p>
<ul>
<li>
<p>hive内置了两个用于<mark>解析json的函数</mark></p>
<pre><code class="language-sql">json_tuple
--是UDTF 表生成函数  输入一行，输出多行  一次提取读个值  可以单独使用 也可以配合lateral view侧视图使用

get_json_object
--是UDF普通函数，输入一行 输出一行 一次只能提取一个值 多次提取多次使用
</code></pre>
</li>
<li>
<p>使用==<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-JSON" target="_blank" class="external-link">JsonSerDe</a> 类解析==，在加载json数据到表中的时候完成解析动作</p>
</li>
</ul>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">--创建表
create table tb_json_test1 (
    json string
);

--加载数据
load data local inpath '/root/hivedata/device.json' into table tb_json_test1;

select * from tb_json_test1;

-- get_json_object UDF函数 最大弊端是一次只能解析提取一个字段
-- ⭐⭐⭐方法1：一个字段一个字段提取
select
    --获取设备名称
    get_json_object(json,&quot;$.device&quot;) as device,
    --获取设备类型
    get_json_object(json,&quot;$.deviceType&quot;) as deviceType,
    --获取设备信号强度
    get_json_object(json,&quot;$.signal&quot;) as signal,
    --获取时间
    get_json_object(json,&quot;$.time&quot;) as stime
from tb_json_test1;

-- json_tuple 这是一个UDTF函数 可以一次解析提取多个字段
-- ⭐⭐⭐方法2
-- 这是udtf，一进多出，这里是一进一出但是也算是一进多出，这里的多就是1
-- 单独使用 解析所有字段
select
    json_tuple(json,&quot;device&quot;,&quot;deviceType&quot;,&quot;signal&quot;,&quot;time&quot;) as (device,deviceType,signal,stime)
from tb_json_test1;

-- ⭐⭐⭐方法3：搭配侧视图使用
select
    json,device,deviceType,signal,stime
from tb_json_test1
         lateral view json_tuple(json,&quot;device&quot;,&quot;deviceType&quot;,&quot;signal&quot;,&quot;time&quot;) b
         as device,deviceType,signal,stime;


--方式2： ⭐⭐⭐方法4：使用JsonSerDe类在建表的时候解析数据
-- 建表的时候直接使用JsonSerDe解析
create table tb_json_test2 (
                               device string,
                               deviceType string,
                               signal double,
                               `time` string
)
    ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
    STORED AS TEXTFILE;

load data local inpath '/root/hivedata/device.json' into table tb_json_test2;

select * from tb_json_test2;
</code></pre>
</li>
</ul>
<p>具体哪个方法好，看具体业务场景</p>
<hr>
<h4 id="06-apache-hive" tabindex="-1">知识点06：Apache Hive--窗口函数--快速理解与语法规则</h4>
<p>1、快速理解窗口函数功能</p>
<ul>
<li>
<p>window function 窗口函数、开窗函数、olap分析函数。</p>
</li>
<li>
<p>窗口：可以理解为操作数据的范围，窗口有大有小，本窗口中操作的数据有多有少。</p>
</li>
<li>
<p>可以简单地解释为类似于聚合函数的计算函数，但是通过GROUP BY子句组合的常规聚合会隐藏正在聚合的各个行，最终输出一行；而<mark>窗口函数聚合后还可以访问当中</mark><mark>的各个行，并且可以将这些行中的某些属性添加到结果集中</mark>。</p>
</li>
</ul>
<p><picture src="/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/9-%E9%99%84%E4%BB%B6/%E9%99%84%E4%BB%B6/8-Apache%20Hive%E5%87%BD%E6%95%B0%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98_image.png" alt="8-Apache Hive函数高阶应用、性能调优_image.png"><source media="(max-width:480px)" srcset="/img/optimized/R2MDTCNyOu-500.webp" type="image/webp">
<source media="(max-width:480px)" srcset="/img/optimized/R2MDTCNyOu-500.jpeg">
<source media="(max-width:1920px)" srcset="/img/optimized/R2MDTCNyOu-700.webp" type="image/webp"><source media="(max-width:1920px)" srcset="/img/optimized/R2MDTCNyOu-700.jpeg"><img class="" src="/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/9-%E9%99%84%E4%BB%B6/%E9%99%84%E4%BB%B6/8-Apache%20Hive%E5%87%BD%E6%95%B0%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98_image.png" alt="8-Apache Hive函数高阶应用、性能调优_image.png" width=""></picture></p>
<pre><code class="language-sql">--建表加载数据
CREATE TABLE employee(
       id int,
       name string,
       deg string,
       salary int,
       dept string
) row format delimited
    fields terminated by ',';

load data local inpath '/root/hivedata/employee.txt' into table employee;

select * from employee;

----sum+group by普通常规聚合操作------------
select dept,sum(salary) as total from employee group by dept;

select id,dept,sum(salary) as total from employee group by dept; --添加id至结果，错误sql

+-------+---------+
| dept  |  total  |
+-------+---------+
| AC    | 60000   |
| TP    | 120000  |
+-------+---------+

----sum+窗口函数聚合操作------------
select id,name,deg,salary,dept,sum(salary) over(partition by dept) as total from employee;

+-------+-----------+----------+---------+-------+---------+
|  id   |   name    |   deg    | salary  | dept  |  total  |
+-------+-----------+----------+---------+-------+---------+
| 1204  | prasanth  | dev      | 30000   | AC    | 60000   |
| 1203  | khalil    | dev      | 30000   | AC    | 60000   |
| 1206  | kranthi   | admin    | 20000   | TP    | 120000  |
| 1202  | manisha   | cto      | 50000   | TP    | 120000  |
| 1201  | gopal     | manager  | 50000   | TP    | 120000  |
+-------+-----------+----------+---------+-------+---------+
</code></pre>
<hr>
<p>2、窗口函数语法规则</p>
<blockquote>
<h5 id="mark-over-mark" tabindex="-1">具有<mark>OVER语句</mark>的函数叫做窗口函数。</h5>
</blockquote>
<pre><code class="language-sql">Function OVER ([PARTITION BY &lt;...&gt;] [ORDER BY &lt;....&gt;] [&lt;window_expression&gt;])

--1、Function可以是下面分类中的任意一个
	--聚合函数：比如sum、max、avg、max、min等
    --排序函数：比如rank、row_number等
    --分析函数：比如lead、lag、first_value等

--2、OVER 窗口函数语法关键字与标识

--3、PARTITION BY &lt;...&gt;功能类似于group by，用于指定分组，相同的分为一组。如果没有指定PARTITION BY，那么整张表的所有行就是一组；

--4、ORDER BY &lt;....&gt; 用于指定每个分组内的数据排序规则 ，默认是升序ASC，支持ASC、DESC；

--5、window_expression window表达式，也叫window子句，用于指定每个窗口中操作的数据范围
	⭐⭐⭐
	有row between 3-5 表示从3到5行
	还有range between 3-5 表示值从3到5
</code></pre>
<ul>
<li>建表加载数据 后续练习使用</li>
</ul>
<pre><code class="language-sql">---建表并且加载数据
create table website_pv_info(
   cookieid string,
   createtime string,   --day
   pv int
) row format delimited
fields terminated by ',';

create table website_url_info (
    cookieid string,
    createtime string,  --访问时间
    url string       --访问页面
) row format delimited
fields terminated by ',';


load data local inpath '/root/hivedata/website_pv_info.txt' into table website_pv_info;
load data local inpath '/root/hivedata/website_url_info.txt' into table website_url_info;

select * from website_pv_info;
select * from website_url_info;
</code></pre>
<hr>
<h4 id="07-apache-hive" tabindex="-1">知识点07：Apache Hive--窗口函数--聚合函数</h4>
<ul>
<li>
<p>语法</p>
<pre><code class="language-sql">sum|max|min|avg  OVER ([PARTITION BY &lt;...&gt;] [ORDER BY &lt;....&gt;] [&lt;window_expression&gt;])
</code></pre>
</li>
<li>
<p>重点：<mark><strong>有PARTITION BY 没有PARTITION BY的区别；有ORDER BY没有ORDER BY的区别</strong></mark>。</p>
<ul>
<li>有没有partition by 影响的是全局聚合 还是分组之后 每个组内聚合。</li>
<li>有没有==<strong>order by的区别</strong>==：
<ul>
<li>没有order by，默认是rows between，首行到最后行，这里的&quot;行&quot;是物理上的行；</li>
<li>有order by，默认是range between，首行到当前行，这里的&quot;行&quot;是逻辑上的行，由字段的值的区间range来划分范围。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">--1、求出每个用户总pv数  sum+group by普通常规聚合操作
select cookieid,sum(pv) as total_pv from website_pv_info group by cookieid;
+-----------+-----------+
| cookieid  | total_pv  |
+-----------+-----------+
| cookie1   | 26        |
| cookie2   | 35        |
+-----------+-----------+


--2、sum+窗口函数 
需要注意：这里都没有使用window子句指定范围，那么默认值是rows还是range呢？？？

--当没有order by也没有window子句的时候，默认是rows between,从第一行到最后一行，即分组内的所有行聚合。
--sum(...) over( )
select cookieid,createtime,pv,
       sum(pv) over() as total_pv
from website_pv_info;

--sum(...) over( partition by... )
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid) as total_pv 
from website_pv_info;


--当有order by但是缺失window子句的时候，默认是range between，为第一行的值到当前行的值构成的区间
--sum(...) over( partition by... order by ... )
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime) as current_total_pv
from website_pv_info;

--上述是根据creattime排序，因此range区间是由createtime的字段值来划分，第一行到当前行。



--下面是根据pv排序，因此range区间是由pv的字段值来划分，第一行到当前行。
--下面两个sql等价
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by pv) as current_total_pv
from website_pv_info; --这属于有order by没有 window子句 默认range

select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by pv range between unbounded preceding and current row) as current_total_pv
from website_pv_info;


--这是手动指定根据rows来划分范围
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by pv rows between unbounded preceding and current row ) as current_total_pv
from website_pv_info;
</code></pre>
</li>
</ul>
<h6 id="czc-hadoop-day08-apache-hive-3-hive-sql-rows-between-range-between-hive-sql-rows-between-range-between-hive-sql-rows-between-range-between" tabindex="-1">⭐⭐⭐<a class="internal-link is-unresolved" href="/404" target="">HIVE SQL 聚合函数与 rows between、 range between详解</a></h6>
<hr>
<h4 id="08-apache-hive-window" tabindex="-1">知识点08：Apache Hive--窗口函数--window子句</h4>
<blockquote>
<h5 id="window-window" tabindex="-1">直译叫做window表达式 ，通俗叫法称之为window子句。</h5>
</blockquote>
<ul>
<li>
<p>功能：控制窗口操作的范围。</p>
</li>
<li>
<p>语法</p>
</li>
</ul>
<pre><code>  unbounded 无边界
  preceding 往前
  following 往后
  unbounded preceding 往前所有行，即初始行
  n preceding 往前n行
  unbounded following 往后所有行，即末尾行
  n following 往后n行
  current row 当前行
   
  语法
  (ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)
  
  (ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)
  (ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING
  
</code></pre>
<ul>
<li>
<p>栗子</p>
<blockquote>
<p>这里以rows between为例来讲解窗口范围的划分，rows表示物理层面上的行，跟字段值没关系。</p>
</blockquote>
<pre><code class="language-sql">--默认从第一行到当前行
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime) as pv1  
from website_pv_info;

--第一行到当前行 等效于rows between不写 默认就是第一行到当前行
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime rows between unbounded preceding and current row) as pv2
from website_pv_info;

--向前3行至当前行
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime rows between 3 preceding and current row) as pv4
from website_pv_info;

--向前3行 向后1行
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime rows between 3 preceding and 1 following) as pv5
from website_pv_info;

--当前行至最后一行
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime rows between current row and unbounded following) as pv6
from website_pv_info;

--第一行到最后一行 也就是分组内的所有行
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime rows between unbounded preceding  and unbounded following) as pv6
from website_pv_info;
</code></pre>
</li>
</ul>
<hr>
<h4 id="09-apache-hive-row-number" tabindex="-1">知识点09：Apache Hive--窗口函数--排序函数（row_number等）</h4>
<p>学的第二类窗口函数</p>
<ul>
<li>
<p>功能：<strong>主要对数据分组排序之后，组内顺序标号。</strong></p>
<ul>
<li>不能用row 和 range between了</li>
</ul>
</li>
<li>
<p>核心函数：<mark><strong>row_number</strong></mark>、rank、dense_rank</p>
</li>
<li>
<p>适合场景：<mark><strong>分组TopN业务分析</strong></mark>（注意哦 不是全局topN）</p>
<ul>
<li>找出我们班男女中最有钱的前两个</li>
<li>找出我们班成绩最高的3个人</li>
</ul>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">SELECT
    cookieid,
    createtime,
    pv,
    RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn1,
    DENSE_RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn2,
    ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv DESC) AS rn3
FROM website_pv_info;
</code></pre>
</li>
</ul>
<pre><code>
![8-Apache Hive函数高阶应用、性能调优_image-2.png](/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/9-%E9%99%84%E4%BB%B6/%E9%99%84%E4%BB%B6/8-Apache%20Hive%E5%87%BD%E6%95%B0%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98_image-2.png)

解释：

![8-Apache Hive函数高阶应用、性能调优_image-3.png](/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/9-%E9%99%84%E4%BB%B6/%E9%99%84%E4%BB%B6/8-Apache%20Hive%E5%87%BD%E6%95%B0%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98_image-3.png)

再来个例子：

![8-Apache Hive函数高阶应用、性能调优_image-4.png](/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/9-%E9%99%84%E4%BB%B6/%E9%99%84%E4%BB%B6/8-Apache%20Hive%E5%87%BD%E6%95%B0%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98_image-4.png)


```sql
  --需求：找出每个用户访问pv最多的Top3 重复并列的不考虑
  SELECT * from
  (SELECT
      cookieid,
      createtime,
      pv,
      ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv DESC) AS seq
  FROM website_pv_info) tmp where tmp.seq &lt;4;
</code></pre>
<ul>
<li>
<p><mark>ntile</mark>函数 <strong>row_number的远房表哥</strong></p>
<ul>
<li>
<p>功能：将分组排序之后的数据分成指定的若干个部分（若干个桶）</p>
</li>
<li>
<p>规则：尽量平均分配 ，优先满足最小的桶，彼此最多不相差1个。</p>
<ul>
<li>第一桶最多，例如7个分三桶就分成 3 2 2</li>
<li>就能去前三分之一就是前三个，取中间三分之一就是中间两个</li>
</ul>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">--把每个分组内的数据分为3桶
SELECT
    cookieid,
    createtime,
    pv,
    NTILE(3) OVER(PARTITION BY cookieid ORDER BY createtime) AS rn2
FROM website_pv_info
ORDER BY cookieid,createtime;

--需求：统计每个用户pv数最多的前3分之1天。
--理解：将数据根据cookieid分 根据pv倒序排序 排序之后分为3个部分 取第一部分
SELECT * from
(SELECT
     cookieid,
     createtime,
     pv,
     NTILE(3) OVER(PARTITION BY cookieid ORDER BY pv DESC) AS rn
 FROM website_pv_info) tmp where rn =1;
</code></pre>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="10-apache-hive-lag-lead" tabindex="-1">知识点10：Apache Hive--窗口函数--lag、lead函数</h4>
<p>窗口分析函数<br>
<picture src="/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/9-%E9%99%84%E4%BB%B6/%E9%99%84%E4%BB%B6/8-Apache%20Hive%E5%87%BD%E6%95%B0%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98_image-5.png" alt="8-Apache Hive函数高阶应用、性能调优_image-5.png"><source media="(max-width:480px)" srcset="/img/optimized/yNDEbdeCQP-500.webp" type="image/webp">
<source media="(max-width:480px)" srcset="/img/optimized/yNDEbdeCQP-500.jpeg">
<source media="(max-width:1920px)" srcset="/img/optimized/yNDEbdeCQP-700.webp" type="image/webp"><source media="(max-width:1920px)" srcset="/img/optimized/yNDEbdeCQP-700.jpeg"><img class="" src="/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/9-%E9%99%84%E4%BB%B6/%E9%99%84%E4%BB%B6/8-Apache%20Hive%E5%87%BD%E6%95%B0%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98_image-5.png" alt="8-Apache Hive函数高阶应用、性能调优_image-5.png" width=""></picture></p>
<pre><code class="language-sql">--LAG 用于统计窗口内往上第n行值
SELECT cookieid,
       createtime,
       url,
       ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn,
       LAG(createtime,1,'1970-01-01 00:00:00') OVER(PARTITION BY cookieid ORDER BY createtime) AS last_1_time,
       LAG(createtime,2) OVER(PARTITION BY cookieid ORDER BY createtime) AS last_2_time
FROM website_url_info;


--LEAD 用于统计窗口内往下第n行值
SELECT cookieid,
       createtime,
       url,
       ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn,
       LEAD(createtime,1,'1970-01-01 00:00:00') OVER(PARTITION BY cookieid ORDER BY createtime) AS next_1_time,
       LEAD(createtime,2) OVER(PARTITION BY cookieid ORDER BY createtime) AS next_2_time
FROM website_url_info;

--FIRST_VALUE 取分组内排序后，截止到当前行，第一个值
SELECT cookieid,
       createtime,
       url,
       ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn,
       FIRST_VALUE(url) OVER(PARTITION BY cookieid ORDER BY createtime) AS first1
FROM website_url_info;

--LAST_VALUE  取分组内排序后，截止到当前行，最后一个值
SELECT cookieid,
       createtime,
       url,
       ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn,
       LAST_VALUE(url) OVER(PARTITION BY cookieid ORDER BY createtime) AS last1
FROM website_url_info;
</code></pre>
<hr>
<h4 id="czc" tabindex="-1">⭐czc的窗口函数总结</h4>
<ul>
<li>关于<code>order by</code>
<ul>
<li>窗口函数中的 <code>ORDER BY</code>（核心作用：定义窗口内的计算顺序）</li>
<li>最后的全局 <code>ORDER BY</code>（核心作用：定义结果集的输出顺序）</li>
</ul>
</li>
</ul>
<h4 id="11-apache-hive-text-orc-parquet" tabindex="-1">知识点11：Apache Hive--文件存储格式（text、ORC、parquet）</h4>
<ul>
<li>
<p>列式存储、行式存储</p>
<ul>
<li>数据最终在文件中底层以什么样的形成保存。</li>
</ul>
</li>
<li>
<p>行列存储的优缺点：</p>
<ul>
<li>行式存储：优点：便于插入、更新</li>
<li>列式存储：优点：便于查询</li>
<li>因为数仓主要工作是对数据进行分析，所以使用大多数时候列式存储，但是<strong>hive默认</strong>是<strong>行式存储</strong></li>
</ul>
</li>
<li>
<p>Hive中表的数据存储格式，不是只支持text文本格式，还支持其他很多格式。</p>
</li>
<li>
<p>hive表的文件格式是如何指定的呢？ 建表的时候通过<mark>STORED AS 语法指定。如果没有指定默认都是textfile</mark>。</p>
</li>
<li>
<p>Hive中主流的几种文件格式。</p>
<ul>
<li>
<p>textfile 文件格式 <mark>默认的</mark></p>
</li>
<li>
<p><mark>ORC</mark>、Parquet(facebook的) 列式存储格式。</p>
<ul>
<li><em>Parquet效率不如ORC，但是比文本格式强一些</em></li>
</ul>
<pre><code>都是列式存储格式，底层是以二进制形式存储。数据存储效率极高，对于查询贼方便。
二进制意味着肉眼无法直接解析，hive可以自解析。
</code></pre>
</li>
</ul>
</li>
</ul>
<p>!<a class="internal-link is-unresolved" href="/404" target="">Pasted image 20250410161300.png</a></p>
<ul>
<li>
<p>栗子</p>
<blockquote>
<p>分别使用3种不同格式存储数据，去HDFS上查看底层文件存储空间的差异。</p>
<p>注意：load是纯复制操作，对于数据的文本文件，必须要先select查询出来再insert，下面例子2后面写了</p>
</blockquote>
<pre><code class="language-sql">--1、创建表，存储数据格式为TEXTFILE
create table log_text (
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;  --如果不写stored as textfile 默认就是textfile

--加载数据
load data local inpath '/root/hivedata/log.data' into table log_text;
</code></pre>
</li>
</ul>
<p>⭐加载后文件大小是'18MB'</p>
<p>--2、创建表，存储数据格式为ORC<br>
create table log_orc(<br>
track_time string,<br>
url string,<br>
session_id string,<br>
referer string,<br>
ip string,<br>
end_user_id string,<br>
city_id string<br>
)<br>
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'<br>
STORED AS orc ;</p>
<p>--向表中插入数据 思考为什么不能使用load命令加载？ 因为load是纯复制移动操作 不会调整文件格式。<br>
insert into table log_orc select * from log_text;<br>
⭐加载后文件大小是'2.78MB'</p>
<p>--3、创建表，存储数据格式为parquet<br>
create table log_parquet(<br>
track_time string,<br>
url string,<br>
session_id string,<br>
referer string,<br>
ip string,<br>
end_user_id string,<br>
city_id string<br>
)<br>
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'<br>
STORED AS PARQUET ;</p>
<p>--向表中插入数据<br>
insert into table log_parquet select * fro m log_text ;<br>
⭐加载进来后文件大小是'12.9MB'<br>
```</p>
<hr>
<h4 id="12-apache-hive-orc-snappy" tabindex="-1">知识点12：Apache Hive--数据压缩和文件格式搭配（ORC+snappy）</h4>
<ul>
<li>
<p>Hive的默认执行引擎是MapReduce，因此通常所说的<mark>Hive压缩指的是MapReduce的压缩</mark>。</p>
</li>
<li>
<p>压缩是指通过<mark>算法对数据进行重新编排</mark>，降低存储空间。无损压缩。</p>
</li>
<li>
<p>MapReduce可以在两个阶段进行数据压缩</p>
<ul>
<li>map的输出
<ul>
<li><mark>减少shuffle的数据量</mark> 提高shuffle时网络IO的效率</li>
</ul>
</li>
<li>reduce的输出
<ul>
<li>减少输出文件的大小 <mark>降低磁盘的存储空间</mark></li>
</ul>
</li>
</ul>
</li>
<li>
<p>压缩的弊端</p>
<ul>
<li><strong>浪费时间</strong></li>
<li><strong>消耗CPU、内存</strong></li>
<li><strong>某些优秀的压缩算法需要钱</strong></li>
</ul>
</li>
<li>
<p>压缩的算法（<mark>推荐使用snappy</mark>）</p>
<ul>
<li>ORC存储会默认使用ZLIB压缩</li>
</ul>
<pre><code>Snappy
org.apache.hadoop.io.compress.SnappyCodec
</code></pre>
</li>
<li>
<p>Hive中压缩的设置：注意 本质还是指的是MapReduce的压缩</p>
<pre><code class="language-sql">--设置Hive的中间压缩 也就是map的输出压缩
1）开启 hive 中间传输数据压缩功能
set hive.exec.compress.intermediate=true;
2）开启 mapreduce 中 map 输出压缩功能
set mapreduce.map.output.compress=true;
3）设置 mapreduce 中 map 输出数据的压缩方式
set mapreduce.map.output.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;

--设置Hive的最终输出压缩，也就是Reduce输出压缩
1）开启 hive 最终输出数据压缩功能
set hive.exec.compress.output=true;
2）开启 mapreduce 最终输出数据压缩
set mapreduce.output.fileoutputformat.compress=true;
3）设置 mapreduce 最终数据输出压缩方式
set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec;
4）设置 mapreduce 最终数据输出压缩为块压缩  还可以指定RECORD
set mapreduce.output.fileoutputformat.compress.type=BLOCK;  
</code></pre>
<pre><code class="language-sql">--设置完毕之后  只有当HiveSQL底层通过MapReduce程序执行 才会涉及压缩。
--已有普通格式的表
select * from student_hdfs ;

--ctas语句
create table student_snappy as select * from student_hdfs ;
</code></pre>
</li>
<li>
<p>在实际开发中，可以根据需求选择不同的文件格式并且搭配不同的压缩算法。可以得到更好的存储效果。</p>
<pre><code class="language-sql">--不指定压缩格式 代表什么呢？
-- orc 存储文件默认采用ZLIB 压缩。比 snappy 压缩的小
STORED AS orc;   --2.78M

--以ORC格式存储 不压缩
STORED AS orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);  --7.69M

--以ORC格式存储  使用snappy压缩
STORED AS orc tblproperties (&quot;orc.compress&quot;=&quot;SNAPPY&quot;); --3.78M
</code></pre>
</li>
</ul>
<hr>
<h4 id="13-apache-hive-fetch-mr" tabindex="-1">知识点13：Apache Hive--通用调优--fetch抓取机制、MR本地模式</h4>
<p>Fetch抓取机制</p>
<ul>
<li>
<p>功能：在执行sql的时候，<mark>能不走</mark> <mark>MapReduce程序处理就尽量不走MapReduce程序处理</mark>。</p>
</li>
<li>
<p>尽量直接去操作数据文件。</p>
</li>
<li>
<p>设置： hive.fetch.task.conversion= more。、</p>
</li>
</ul>
<p><strong>在下述3种情况下 sql不走mr程序</strong></p>
<pre><code class="language-sql">--全局查找
select * from student;
--字段查找
select num,name from student;
--limit 查找
select num,name from student limit 2;
</code></pre>
<hr>
<p>MapReduce本地模式</p>
<ul>
<li>
<p>功能：如果非要执行<mark>MapReduce程序，能够本地执行的，尽量不提交yarn上执行</mark>。</p>
</li>
<li>
<p>默认是关闭的。意味着只要走MapReduce就提交yarn执行。</p>
<pre><code>mapreduce.framework.name = local 本地模式
mapreduce.framework.name = yarn 集群模式 
</code></pre>
</li>
<li>
<p>Hive提供了一个参数，自动切换MapReduce程序为本地模式，如果不满足条件，就执行yarn模式。</p>
<pre><code class="language-sql">set hive.exec.mode.local.auto = true;
 
--3个条件必须都满足 自动切换本地模式
The total input size of the job is lower than: hive.exec.mode.local.auto.inputbytes.max (128MB by default)  --数据量小于128M

The total number of map-tasks is less than: hive.exec.mode.local.auto.tasks.max (4 by default)  --maptask个数少于4个

The total number of reduce tasks required is 1 or 0.  --reducetask个数是0 或者 1
</code></pre>
</li>
</ul>
<p><strong>本地模式不能本质解决问题，用MR跑hive就是慢，建议切换引擎，建议用spark、tez</strong></p>
<ul>
<li>
<p>切换Hive的执行引擎</p>
<pre><code>WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.

如果针对Hive的调优依然无法满足你的需求 还是效率低， 尝试使用spark计算引擎 或者Tez.
</code></pre>
</li>
</ul>
<hr>
<h4 id="14-apache-hive-join" tabindex="-1">知识点14：Apache Hive--通用调优--join优化</h4>
<blockquote>
<p>底层还是MapReduce的 join 优化</p>
</blockquote>
<div class="callout" data-callout="tip"><div class="callout-title"><div class="callout-title-inner"> 下面的总结：<br></div></div>
<div class="callout-content">
<p>
对<strong>join的优化</strong>最终都是往<strong>Map端的join</strong>靠近，避免走reduce的join<br>
实在不行只能用reduce的join慢慢跑，至少能跑出来</p>
</div></div>
<h5 id="map-join" tabindex="-1">MapJoin</h5>
<ul>
<li>
<p>map join</p>
<ul>
<li>
<p>是<strong>MAP阶段的join，没有Reduce阶段，只在map阶段完成数据join动作</strong></p>
</li>
<li>
<p>适合于<mark>小表join大表</mark>或者<mark>小表Join小表</mark></p>
</li>
<li>
<p>核心：<strong>必须有一张表是所谓的小表 smalltable</strong>，因为小表要放入Distributed Cache</p>
</li>
<li>
<p>所以当Distributed Cache放不下的时候就自动走Reduce段Join</p>
</li>
<li>
<p>因为没有Reduce过程，就没有了shuffle过程，效率大大提高</p>
</li>
</ul>
<p>![image-20210921215332809](/img/user/czc知识库/计算机/Hadoop技术栈/资料讲义/源/day08--Apache Hive函数高阶应用、性能调优/1、笔记、总结/hadoop离线day08-Apache Hive函数高阶应用、性能调优.assets/image-20210921215332809.png)</p>
<pre><code class="language-shell"><a class="tag" onclick="toggleTagSearch(this)" data-content="#是否开启自动转为mapjoin">#是否开启自动转为mapjoin</a> 在满足条件的情况下 默认true
hive.auto.convert.join=true

Hive老版本
<a class="tag" onclick="toggleTagSearch(this)" data-content="#如果参与的一个表大小满足条件">#如果参与的一个表大小满足条件</a> 转换为map join
hive.mapjoin.smalltable.filesize=25000000  


Hive2.0之后版本
<a class="tag" onclick="toggleTagSearch(this)" data-content="#是否启用基于输入文件的大小，将reduce">#是否启用基于输入文件的大小，将reduce</a> join转化为Map join的优化机制。假设参与join的表(或分区)有N个，如果打开这个参数，并且有N-1个表(或分区)的大小总和小于hive.auto.convert.join.noconditionaltask.size参数指定的值，那么会直接将join转为Map join。
hive.auto.convert.join.noconditionaltask=true 
hive.auto.convert.join.noconditionaltask.size=512000000 
</code></pre>
</li>
</ul>
<h5 id="reduce-join" tabindex="-1">ReduceJoin</h5>
<ul>
<li>reduce join (commonjoin)
<ul>
<li>适合于大表Join大表</li>
</ul>
</li>
</ul>
<h5 id="bucket-join" tabindex="-1">BucketJoin</h5>
<ul>
<li>bucket join
<ul>
<li>
<p>适合于大表Join大表</p>
</li>
<li>
<p>方式1：Bucktet Map Join</p>
<ul>
<li>语法: <code>clustered by colName</code>(参与join的字段)</li>
<li>参数: <code>set hive.optimize.bucketmapjoin = true</code></li>
<li>要求: <strong>分桶字段 = Join字段</strong> ，分桶的个数相等或者成倍数，必须是在map join中</li>
</ul>
</li>
<li>
<p>方式2：<code>Sort Merge Bucket Join</code>（SMB）<strong>桶表的升级操作</strong>（排序的桶表）</p>
<ul>
<li>基于有序的数据Join</li>
<li>语法:<code>clustered by colName sorted by (colName)</code></li>
<li>参数<br>
<code>set hive.optimize.bucketmapjoin = true;</code><br>
<code>set hive.auto.convert.sortmerge.join=true;</code><br>
<code>set hive.optimize.bucketmapjoin.sortedmerge = true;</code><br>
<code>set hive.auto.convert.sortmerge.join.noconditionaltask=true;</code></li>
<li>要求: 分桶字段 = Join字段 = 排序字段,分桶的个数相等或者成倍数</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="15-apache-hive" tabindex="-1">知识点15：Apache Hive--通用调优--数据倾斜优化</h4>
<p>数据倾斜：数据分布的不平均现象</p>
<p><picture src="/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/9-%E9%99%84%E4%BB%B6/%E9%99%84%E4%BB%B6/8-Apache%20Hive%E5%87%BD%E6%95%B0%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98_image-7.png" alt="8-Apache Hive函数高阶应用、性能调优_image-7.png"><source media="(max-width:480px)" srcset="/img/optimized/8i0akVpc_U-500.webp" type="image/webp">
<source media="(max-width:480px)" srcset="/img/optimized/8i0akVpc_U-500.jpeg">
<source media="(max-width:1920px)" srcset="/img/optimized/8i0akVpc_U-700.webp" type="image/webp"><source media="(max-width:1920px)" srcset="/img/optimized/8i0akVpc_U-700.jpeg"><img class="" src="/img/user/czc%E7%9F%A5%E8%AF%86%E5%BA%93/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/9-%E9%99%84%E4%BB%B6/%E9%99%84%E4%BB%B6/8-Apache%20Hive%E5%87%BD%E6%95%B0%E9%AB%98%E9%98%B6%E5%BA%94%E7%94%A8%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98_image-7.png" alt="8-Apache Hive函数高阶应用、性能调优_image-7.png" width=""></picture></p>
<ul>
<li>
<p>group by数据倾斜</p>
<ul>
<li>
<p>方案一：开启Map端聚合</p>
<pre><code class="language-shell">hive.map.aggr=true;
<a class="tag" onclick="toggleTagSearch(this)" data-content="#是否在Hive">#是否在Hive</a> Group By 查询中使用map端聚合。
<a class="tag" onclick="toggleTagSearch(this)" data-content="#这个设置可以将顶层的部分聚合操作放在Map阶段执行，从而减轻清洗阶段数据传输和Reduce阶段的执行时间，提升总体性能。但是指标不治本。">#这个设置可以将顶层的部分聚合操作放在Map阶段执行，从而减轻清洗阶段数据传输和Reduce阶段的执行时间，提升总体性能。但是指标不治本。</a>
</code></pre>
</li>
<li>
<p>方案二：实现随机分区</p>
<pre><code>实现随机分区
select * from table distribute by rand();
</code></pre>
</li>
<li>
<p>方案三：数据倾斜时<mark>自动负载均衡</mark></p>
<pre><code class="language-shell">hive.groupby.skewindata=true;

<a class="tag" onclick="toggleTagSearch(this)" data-content="#开启该参数以后，当前程序会自动通过两个MapReduce来运行">#开启该参数以后，当前程序会自动通过两个MapReduce来运行</a>

<a class="tag" onclick="toggleTagSearch(this)" data-content="#第一个MapReduce自动进行随机分布到Reducer中，每个Reducer做部分聚合操作，输出结果">#第一个MapReduce自动进行随机分布到Reducer中，每个Reducer做部分聚合操作，输出结果</a>

<a class="tag" onclick="toggleTagSearch(this)" data-content="#第二个MapReduce将上一步聚合的结果再按照业务（group">#第二个MapReduce将上一步聚合的结果再按照业务（group</a> by key）进行处理，保证相同的分布到一起，最终聚合得到结果
</code></pre>
</li>
</ul>
</li>
<li>
<p>join数据倾斜</p>
<ul>
<li>
<p>方案一：提前过滤，将大数据变成小数据，实现Map Join</p>
</li>
<li>
<p>方案二：使用Bucket Join</p>
</li>
<li>
<p>方案三：使用Skew Join</p>
<pre><code class="language-shell"><a class="tag" onclick="toggleTagSearch(this)" data-content="#将Map">#将Map</a> Join和Reduce Join进行合并，如果某个值出现了数据倾斜，就会将产生数据倾斜的数据单独使用Map Join来实现

<a class="tag" onclick="toggleTagSearch(this)" data-content="#其他没有产生数据倾斜的数据由Reduce">#其他没有产生数据倾斜的数据由Reduce</a> Join来实现，这样就避免了Reduce Join中产生数据倾斜的问题

<a class="tag" onclick="toggleTagSearch(this)" data-content="#最终将Map">#最终将Map</a> Join的结果和Reduce Join的结果进行Union合并

<a class="tag" onclick="toggleTagSearch(this)" data-content="#开启运行过程中skewjoin">#开启运行过程中skewjoin</a>
set hive.optimize.skewjoin=true;
<a class="tag" onclick="toggleTagSearch(this)" data-content="#如果这个key的出现的次数超过这个范围">#如果这个key的出现的次数超过这个范围</a>
set hive.skewjoin.key=100000;
<a class="tag" onclick="toggleTagSearch(this)" data-content="#在编译时判断是否会产生数据倾斜">#在编译时判断是否会产生数据倾斜</a>
set hive.optimize.skewjoin.compiletime=true;
set hive.optimize.union.remove=true;
<a class="tag" onclick="toggleTagSearch(this)" data-content="#如果Hive的底层走的是MapReduce，必须开启这个属性，才能实现不合并">#如果Hive的底层走的是MapReduce，必须开启这个属性，才能实现不合并</a>
set mapreduce.input.fileinputformat.input.dir.recursive=true;

</code></pre>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="16-apache-hive-mr-task" tabindex="-1">知识点16：Apache Hive--通用调优--MR程序task个数调整</h4>
<ul>
<li>
<p>maptask个数</p>
<ul>
<li>
<p>如果是在MapReduce中 maptask是通过<mark>逻辑切片</mark>机制决定的。</p>
</li>
<li>
<p>但是在hive中，影响的因素很多。比如逻辑切片机制，文件是否压缩、压缩之后是否支持切割。</p>
</li>
<li>
<p>因此在<mark>Hive中，调整MapTask的个数，直接去HDFS调整文件的大小和个数，效率较高</mark>。</p>
<pre><code>如果小文件多，就进行小文件的合并  合并的大小最好=block size
如果大文件多，就调整blocl size
</code></pre>
</li>
</ul>
</li>
<li>
<p>reducetask个数</p>
<ul>
<li>
<p>如果在MapReduce中，通过代码可以直接指定 job.setNumReduceTasks(N)</p>
</li>
<li>
<p>在Hive中，reducetask个数受以下几个条件控制的</p>
<pre><code class="language-sql">（1）每个 Reduce 处理的数据量默认是 256MB
hive.exec.reducers.bytes.per.reducer=256000000
（2）每个任务最大的 reduce 数，默认为 1009
hive.exec.reducsers.max=1009
（3）mapreduce.job.reduces
该值默认为-1，由 hive 自己根据任务情况进行判断。


--如果用户用户不设置 hive将会根据数据量或者sql需求自己评估reducetask个数。
--用户可以自己通过参数设置reducetask的个数
  set mapreduce.job.reduces = N
--用户设置的不一定生效，如果用户设置的和sql执行逻辑有冲突，比如order by，在sql编译期间，hive又会将reducetask设置为合理的个数。  

Number of reduce tasks determined at compile time: 1
</code></pre>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="17-apache-hive" tabindex="-1">知识点17：Apache Hive--通用调优--执行计划</h4>
<ul>
<li>
<p>通过执行计划可以看出<mark>hive接下来是如何打算执行这条sql的</mark>。</p>
</li>
<li>
<p>语法格式：explain + sql语句</p>
</li>
<li>
<p>栗子</p>
<pre><code class="language-sql">explain select * from student;

+----------------------------------------------------+
|                      Explain                       |
+----------------------------------------------------+
| STAGE DEPENDENCIES:                                |
|   Stage-0 is a root stage                          |
|                                                    |
| STAGE PLANS:                                       |
|   Stage: Stage-0                                   |
|     Fetch Operator                                 |
|       limit: -1                                    |
|       Processor Tree:                              |
|         TableScan                                  |
|           alias: student                           |
|           Statistics: Num rows: 1 Data size: 5260 Basic stats: COMPLETE Column stats: NONE |
|           Select Operator                          |
|             expressions: num (type: int), name (type: string), sex (type: string), age (type: int), dept (type: string) |
|             outputColumnNames: _col0, _col1, _col2, _col3, _col4 |
|             Statistics: Num rows: 1 Data size: 5260 Basic stats: COMPLETE Column stats: NONE |
|             ListSink                               |
|                                                    |
+----------------------------------------------------+
</code></pre>
</li>
</ul>
<hr>
<h4 id="18-apache-hive" tabindex="-1">知识点18：Apache Hive--通用调优--并行机制、推测执行机制</h4>
<ul>
<li>
<p>并行执行机制</p>
<ul>
<li>
<p>如果hivesql的底层某些stage阶段可以并行执行，就可以提高执行效率。</p>
</li>
<li>
<p>前提是<mark>stage之间没有依赖</mark> 并行的弊端是瞬时服务器压力变大。</p>
</li>
<li>
<p>参数</p>
<pre><code class="language-sql">set hive.exec.parallel=true; --是否并行执行作业。适用于可以并行运行的 MapReduce 作业，例如在多次插入期间移动文件以插入目标
set hive.exec.parallel.thread.number=16; --最多可以并行执行多少个作业。默认为8。
</code></pre>
</li>
</ul>
</li>
<li>
<p>Hive的严格模式</p>
<ul>
<li>
<p>注意。不要和动态分区的严格模式搞混淆。</p>
</li>
<li>
<p>这里的严格模式指的是开启之后 <mark>hive会禁止一些用户都影响不到的错误包括效率低下的操作</mark>，不允许运行一些有风险的查询。</p>
</li>
<li>
<p>设置</p>
<pre><code class="language-sql">set hive.mapred.mode = strict --默认是严格模式  nonstrict
</code></pre>
</li>
<li>
<p>解释</p>
<pre><code>1、如果是分区表，没有where进行分区裁剪 禁止执行
2、order by语句必须+limit限制
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>推测执行机制</strong></p>
<ul>
<li>MapReduce中task的一个机制。</li>
<li>功能：
<ul>
<li>一个job底层可能有多个task执行，如果某些拖后腿的task执行慢，可能会导致最终job失败。</li>
<li>所谓的<mark>推测执行机制就是通过算法找出拖后腿的task,为其启动备份的task</mark>。</li>
<li>两个task同时处理一份数据，谁先处理完，谁的结果作为最终结果。</li>
</ul>
</li>
<li>推测执行机制默认是开启的，但是在企业生产环境中<mark>建议关闭</mark>。</li>
</ul>
</li>
</ul>
</main>
<aside>
<div class="sidebar">
<div class="sidebar-container">
<div class="toc">
<div class="toc-title-container">
<div class="toc-title">
On this page
</div>
</div>
<div class="toc-container">
<nav class="toc">
<ol>
<li><a href="#hadoop-day08-apache-hive">hadoop离线day08-Apache Hive函数高阶应用、性能调优</a>
<ol>
<li><a href="#今日课程学习目标">今日课程学习目标</a>
</li>
<li><a href="#今日课程内容大纲">今日课程内容大纲</a>
</li>
<li><a href="#01-apache-hive-explode-udtf">知识点01：Apache Hive--explode函数的使用与限制（UDTF表生成函数）</a>
</li>
<li><a href="#02-apache-hive-lateral-view">知识点02：Apache Hive--lateral view侧视图的使用</a>
</li>
<li><a href="#03-apache-hive-collect-list-concat-ws">知识点03：Apache Hive--行列转换--多行转单列（collect_list、concat_ws）</a>
</li>
<li><a href="#04-apache-hive-explode-lateral-view">知识点04：Apache Hive--行列转换--单列转多行（explode、lateral view）</a>
</li>
<li><a href="#05-apache-hive-json">知识点05：Apache Hive--json格式数据处理</a>
</li>
<li><a href="#06-apache-hive">知识点06：Apache Hive--窗口函数--快速理解与语法规则</a>
<ol>
<li><a href="#mark-over-mark">具有OVER语句的函数叫做窗口函数。</a>
</li>
</ol>
</li>
<li><a href="#07-apache-hive">知识点07：Apache Hive--窗口函数--聚合函数</a>
<ol>
<li><a href="#czc-hadoop-day08-apache-hive-3-hive-sql-rows-between-range-between-hive-sql-rows-between-range-between-hive-sql-rows-between-range-between">⭐⭐⭐[[czc知识库/计算机/Hadoop技术栈/资料讲义/源/day08-+Apache Hive/3、资料/HIVE SQL 聚合函数与 rows between、 range between详解/HIVE SQL 聚合函数与 rows between、 range between详解|HIVE SQL 聚合函数与 rows between、 range between详解]]</a>
</li>
</ol>
</li>
<li><a href="#08-apache-hive-window">知识点08：Apache Hive--窗口函数--window子句</a>
<ol>
<li><a href="#window-window">直译叫做window表达式 ，通俗叫法称之为window子句。</a>
</li>
</ol>
</li>
<li><a href="#09-apache-hive-row-number">知识点09：Apache Hive--窗口函数--排序函数（row_number等）</a>
</li>
<li><a href="#10-apache-hive-lag-lead">知识点10：Apache Hive--窗口函数--lag、lead函数</a>
</li>
<li><a href="#czc">⭐czc的窗口函数总结</a>
</li>
<li><a href="#11-apache-hive-text-orc-parquet">知识点11：Apache Hive--文件存储格式（text、ORC、parquet）</a>
</li>
<li><a href="#12-apache-hive-orc-snappy">知识点12：Apache Hive--数据压缩和文件格式搭配（ORC+snappy）</a>
</li>
<li><a href="#13-apache-hive-fetch-mr">知识点13：Apache Hive--通用调优--fetch抓取机制、MR本地模式</a>
</li>
<li><a href="#14-apache-hive-join">知识点14：Apache Hive--通用调优--join优化</a>
<ol>
<li><a href="#map-join">MapJoin</a>
</li>
<li><a href="#reduce-join">ReduceJoin</a>
</li>
<li><a href="#bucket-join">BucketJoin</a>
</li>
</ol>
</li>
<li><a href="#15-apache-hive">知识点15：Apache Hive--通用调优--数据倾斜优化</a>
</li>
<li><a href="#16-apache-hive-mr-task">知识点16：Apache Hive--通用调优--MR程序task个数调整</a>
</li>
<li><a href="#17-apache-hive">知识点17：Apache Hive--通用调优--执行计划</a>
</li>
<li><a href="#18-apache-hive">知识点18：Apache Hive--通用调优--并行机制、推测执行机制</a>
</li>
</ol>
</li>
</ol>
</nav>
</div>
</div>
<div class="backlinks">
<div class="backlink-title" style="margin:4px 0!important">Pages mentioning this page</div>
<div class="backlink-list"><div class="backlink-card"><i icon-name="link"></i><a href="/czc知识库/计算机/Hadoop技术栈/Hadoop技术栈/" data-note-icon="" class="backlink">Hadoop技术栈</a>
</div></div>
</div>
</div>
</div>
</aside>
<style>#tooltip-wrapper{background:var(--background-primary);padding:1em;border-radius:4px;overflow:hidden;position:fixed;width:80%;max-width:400px;height:auto;max-height:300px;font-size:.8em;box-shadow:0 5px 10px rgba(0,0,0,.1);opacity:0;transition:opacity .1s;unicode-bidi:plaintext;overflow-y:scroll;z-index:10}#tooltip-wrapper:after{content:"";position:absolute;z-index:1;bottom:0;left:0;pointer-events:none;width:100%;unicode-bidi:plaintext;height:75px}</style>
<div style="opacity:0;display:none" id="tooltip-wrapper">
<div id="tooltip-content">
</div>
</div>
<iframe style="display:none;height:0;width:0" id="link-preview-iframe" src="">
</iframe>
<script>var opacityTimeout,contentTimeout,transitionDurationMs=100,iframe=document.getElementById("link-preview-iframe"),tooltipWrapper=document.getElementById("tooltip-wrapper"),tooltipContent=document.getElementById("tooltip-content"),linkHistories={};function hideTooltip(){opacityTimeout=setTimeout((function(){tooltipWrapper.style.opacity=0,contentTimeout=setTimeout((function(){tooltipContent.innerHTML="",tooltipWrapper.style.display="none"}),transitionDurationMs+1)}),transitionDurationMs)}function showTooltip(t){var e=t.target,o=e.getClientRects()[e.getClientRects().length-1],i=window.pageYOffset||document.documentElement.scrollTop,n=t.target.getAttribute("href");if(-1===n.indexOf("http")||-1!==n.indexOf(window.location.host)){let t=n.split("#")[0];linkHistories[t]?(tooltipContent.innerHTML=linkHistories[t],tooltipWrapper.style.display="block",setTimeout((function(){if(tooltipWrapper.style.opacity=1,-1!=n.indexOf("#")){let t=n.split("#")[1];const e=tooltipWrapper.querySelector(`[id='${t}']`);e.classList.add("referred"),e.scrollIntoView({behavior:"smooth"},!0)}else tooltipWrapper.scroll(0,0)}),1)):(iframe.src=t,iframe.onload=function(){tooltipContentHtml="",tooltipContentHtml+='<div style="font-weight: bold; unicode-bidi: plaintext;">'+iframe.contentWindow.document.querySelector("h1").innerHTML+"</div>",tooltipContentHtml+=iframe.contentWindow.document.querySelector(".content").innerHTML,tooltipContent.innerHTML=tooltipContentHtml,linkHistories[t]=tooltipContentHtml,tooltipWrapper.style.display="block",tooltipWrapper.scrollTop=0,setTimeout((function(){if(tooltipWrapper.style.opacity=1,-1!=n.indexOf("#")){let t=n.split("#")[1];const e=tooltipWrapper.querySelector(`[id='${t}']`);e.classList.add("referred"),console.log(e),e.scrollIntoView({behavior:"smooth"},!0)}else tooltipWrapper.scroll(0,0)}),1)}),tooltipWrapper.style.left=o.left-tooltipWrapper.offsetWidth/2+o.width/2+"px",window.innerHeight-o.top<tooltipWrapper.offsetHeight?tooltipWrapper.style.top=o.top+i-tooltipWrapper.offsetHeight-10+"px":window.innerHeight-o.top>tooltipWrapper.offsetHeight&&(tooltipWrapper.style.top=o.top+i+35+"px"),o.left+o.width/2<tooltipWrapper.offsetWidth/2?tooltipWrapper.style.left="10px":document.body.clientWidth-o.left-o.width/2<tooltipWrapper.offsetWidth/2&&(tooltipWrapper.style.left=document.body.clientWidth-tooltipWrapper.offsetWidth-20+"px")}}function setupListeners(t){t.addEventListener("mouseleave",(function(t){hideTooltip()})),tooltipWrapper.addEventListener("mouseleave",(function(t){hideTooltip()})),t.addEventListener("mouseenter",(function(t){clearTimeout(opacityTimeout),clearTimeout(contentTimeout),showTooltip(t)})),tooltipWrapper.addEventListener("mouseenter",(function(t){clearTimeout(opacityTimeout),clearTimeout(contentTimeout)}))}window.addEventListener("load",(function(t){document.querySelectorAll(".internal-link").forEach(setupListeners),document.querySelectorAll(".backlink-card a").forEach(setupListeners)}))</script>
<script>window.location.hash&&document.getElementById(window.location.hash.slice(1)).classList.add("referred"),window.addEventListener("hashchange",(e=>{const t=e.oldURL.split("#");t[1]&&document.getElementById(t[1]).classList.remove("referred");const n=e.newURL.split("#");n[1]&&document.getElementById(n[1]).classList.add("referred")}),!1);const url_parts=window.location.href.split("#"),url=url_parts[0],referrence=url_parts[1];document.querySelectorAll(".cm-s-obsidian > *[id]").forEach((function(e){e.ondblclick=function(e){const t=url+"#"+e.target.id;navigator.clipboard.writeText(t)}}))</script>
<script src="https://fastly.jsdelivr.net/npm/luxon@3.2.1/build/global/luxon.min.js"></script>
<script defer="defer">TIMESTAMP_FORMAT="MMM dd, yyyy h:mm a",document.querySelectorAll(".human-date").forEach((function(e){date=e.getAttribute("data-date")||e.innerText,parsed_date=luxon.DateTime.fromISO(date),null!=parsed_date.invalid&&(parsed_date=luxon.DateTime.fromSQL(date)),null!=parsed_date.invalid&&(parsed_date=luxon.DateTime.fromHTML(date)),e.innerHTML=parsed_date.toFormat(TIMESTAMP_FORMAT)}))</script>
<script>lucide.createIcons({attrs:{class:["svg-icon"]}})</script>
</body>
</html>
